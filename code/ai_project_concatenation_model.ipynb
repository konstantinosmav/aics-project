{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing modules I will use\n",
    "import pandas as pd\n",
    "import json\n",
    "from collections import defaultdict, Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "from itertools import combinations\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import numpy as np\n",
    "import ast\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from gensim.models import KeyedVectors\n",
    "import string\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "device = torch.device('cuda:1') #if torch.cuda.is_available() else 'cpu')\n",
    "from PIL import Image\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/df_subm.csv')\n",
    "\n",
    "val_df = pd.read_csv('../data/val_subm.csv')\n",
    "test_df = pd.read_csv('../data/test_subm.csv')\n",
    "train_df = pd.read_csv('../data/train_subm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have the tensors saved locally, I can upload them on my repo too\n",
    "# I usually start things from this point\n",
    "\n",
    "val_tensors = torch.load('../data/val_subm.pt')\n",
    "test_tensors = torch.load('../data/test_subm.pt')\n",
    "train_tensors = torch.load('../data/train_subm.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv('/home/gusmavko@GU.GU.SE/aics-project/data/half_data.csv')\n",
    "#\n",
    "#train_df = pd.read_csv(\"/home/gusmavko@GU.GU.SE/aics-project/data/train_half.csv\")\n",
    "#test_df = pd.read_csv(\"/home/gusmavko@GU.GU.SE/aics-project/data/test_half.csv\")\n",
    "#val_df = pd.read_csv(\"/home/gusmavko@GU.GU.SE/aics-project/data/val_half.csv\")\n",
    "\n",
    "df['genres']=df['genres'].apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "train_df['genres']=train_df['genres'].apply(lambda x: ast.literal_eval(x))\n",
    "test_df['genres']=test_df['genres'].apply(lambda x: ast.literal_eval(x))\n",
    "val_df['genres']=val_df['genres'].apply(lambda x: ast.literal_eval(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['production_companies']=df['production_companies'].apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "train_df['production_companies']=train_df['production_companies'].apply(lambda x: ast.literal_eval(x))\n",
    "test_df['production_companies']=test_df['production_companies'].apply(lambda x: ast.literal_eval(x))\n",
    "val_df['production_companies']=val_df['production_companies'].apply(lambda x: ast.literal_eval(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['production_companies']=train_df['production_companies'].apply(lambda x: (' ').join(x))\n",
    "test_df['production_companies']=test_df['production_companies'].apply(lambda x: (' ').join(x))\n",
    "val_df['production_companies']=val_df['production_companies'].apply(lambda x: (' ').join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "# We need to pass a list of lists of the labels to get a one-hot encoding for the train, the test and the validation dataset.\n",
    "\n",
    "# Fitting the multilabel binarizer to the labels available.\n",
    "#print(mlb.fit(df['genres'].tolist()))\n",
    "\n",
    "train_labels = mlb.fit_transform(train_df['genres'].tolist())\n",
    "test_labels = mlb.fit_transform(test_df['genres'].tolist())\n",
    "val_labels = mlb.fit_transform(val_df['genres'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This function creates a dictionary that has the number of movie labels as a key \n",
    "# and the number of occurrences of this number of labels as a value.\n",
    "\n",
    "def plot_num_labels_movies(genre_lists):\n",
    "    dic = {}\n",
    "    for genre_list in genre_lists:\n",
    "        if len(genre_list) in dic:\n",
    "            dic[len(genre_list)] += 1\n",
    "            \n",
    "        else:\n",
    "            dic[len(genre_list)] = 1\n",
    "    return dic \n",
    "\n",
    "plot_dic = plot_num_labels_movies(df.genres.tolist())\n",
    "plt.bar(*zip(*plot_dic.items()))\n",
    "plt.title('Occurrences of different number of labels in the dataset')\n",
    "plt.xlabel('Number of labels')\n",
    "plt.ylabel('Number of occurrences')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "swords = stopwords.words('english')\n",
    "reg_ex_tokenizer = RegexpTokenizer(r'\\w+')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "#stemmer = Porter\n",
    "\n",
    "# Thought about using different cleaning up methods for titles and summary.\n",
    "def clean(text,is_for_summary=True):\n",
    "    # removing punctuation and digits\n",
    "    text = ''.join([c for c in text if c not in string.punctuation and not c.isdigit()])\n",
    "    # tokenizing with the RegexpTokenizer and lowercasing the text\n",
    "    text = reg_ex_tokenizer.tokenize(text.lower())\n",
    "    # removing stop words if cleaning is applied on the overview - if we clean up the titles we keep the stop words since certain titles contain solely stop words(e.g. 'Who am I?').\n",
    "    # maybe I should filter out stop words in titles too and only keep them when there are no other words.\n",
    "    if is_for_summary:        \n",
    "        text = [w for w in text if w not in swords]\n",
    "    # lemmatizing the words with the WordNetLemmatizer and using join to put them back into a string\n",
    "    text = ' '.join([lemmatizer.lemmatize(w) for w in text])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning up the orignal dataframe's overview and original_title column.\n",
    "#df['overview'] = df['overview'].apply(lambda x: clean(x))\n",
    "#df['original_title'] = df['original_title'].apply(lambda x: clean(x))\n",
    "#df['production_companies'] =  df['production_companies'].apply(lambda x: clean(x))\n",
    "# Cleaning up the overview column.\n",
    "train_df['overview'] = train_df['overview'].apply(lambda x: clean(x))\n",
    "test_df['overview'] = test_df['overview'].apply(lambda x: clean(x))\n",
    "val_df['overview'] = val_df['overview'].apply(lambda x: clean(x))\n",
    "\n",
    "# Cleaning up the original title column.\n",
    "train_df['original_title'] = train_df['original_title'].apply(lambda x: clean(x,False))\n",
    "test_df['original_title'] = test_df['original_title'].apply(lambda x: clean(x,False))\n",
    "val_df['original_title'] = val_df['original_title'].apply(lambda x: clean(x,False))\n",
    "\n",
    "# Cleaning up the production companies column.\n",
    "train_df['production_companies'] = train_df['production_companies'].apply(lambda x: clean(x))\n",
    "test_df['production_companies'] = test_df['production_companies'].apply(lambda x: clean(x))\n",
    "val_df['production_companies'] = val_df['production_companies'].apply(lambda x: clean(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the three columns containing textual data into one to cover all possible occurrences for words.\n",
    "df_tit_overview = df[['original_title','overview','production_companies']] \n",
    "df_vocab=df_tit_overview.stack().reset_index()\n",
    "# Getting length of sequences for padding\n",
    "max_len_for_padding = df_vocab[0].map(len).max()\n",
    "#max_len_title_overview = df['title_and_overview'].map(len).max()\n",
    "embedding_dim = 300\n",
    "tokenizer = Tokenizer(num_words=50000, lower=True)\n",
    "tokenizer.fit_on_texts(df_vocab[0].values)\n",
    "# Creating a word to integer dictionary for my vocab.\n",
    "word_index = tokenizer.word_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70173"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gusmavko@GU.GU.SE/.local/lib/python3.7/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "def embed_vocab(pretrained_dict, vocab, emb_size):\n",
    "    # creating a dictionary with keys the words and values the pretrained word vectors. \n",
    "    embeddings_index = {}\n",
    "    for w in pretrained_dict.wv.vocab:\n",
    "        embeddings_index[w] = pretrained_dict.word_vec(w)     \n",
    "    # instantiating matrix with shape (vocab + 1, 300)\n",
    "    embedding_matrix = 1 * np.random.randn(len(vocab)+1, emb_size)\n",
    "    # looking up the words in my vocab\n",
    "    for word, i in vocab.items():\n",
    "        i-=1\n",
    "        # getting the pretrained vector for the corresponding word\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # adding the vector to the matrix\n",
    "            embedding_matrix[i] = embedding_vector                      \n",
    "    del(embeddings_index)\n",
    "        \n",
    "    return embedding_matrix\n",
    "\n",
    "# need to download the pretrained vectors file\n",
    "\n",
    "w2v_dic = KeyedVectors.load_word2vec_format(\"/home/gusmavko@GU.GU.SE/aics-project/data/GoogleNews-vectors-negative300.bin\", binary=True)\n",
    "word2vec_matrix = embed_vocab(w2v_dic, word_index, 300)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train overview shape:(19700, 800), Test overview shape:(5507, 800), Validation overview shape:(2195, 800)\n"
     ]
    }
   ],
   "source": [
    "# Transforming each word token from the textual data into the corresponding index. \n",
    "# TODO maybe when padding do I need to pad the titles to the same length as the overview \n",
    "\n",
    "X_overview_train = tokenizer.texts_to_sequences(train_df['overview'].values)\n",
    "X_overview_train = pad_sequences(X_overview_train, maxlen=max_len_for_padding)\n",
    "\n",
    "X_overview_test = tokenizer.texts_to_sequences(test_df['overview'].values)\n",
    "X_overview_test = pad_sequences(X_overview_test, maxlen=max_len_for_padding)\n",
    "\n",
    "X_overview_val = tokenizer.texts_to_sequences(val_df['overview'].values)\n",
    "X_overview_val = pad_sequences(X_overview_val, maxlen=max_len_for_padding)\n",
    "\n",
    "print(f'Train overview shape:{X_overview_train.shape}, Test overview shape:{X_overview_test.shape}, Validation overview shape:{X_overview_val.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train title shape:(19700, 93), Test overview shape:(5507, 93), Validation overview shape:(2195, 93)\n"
     ]
    }
   ],
   "source": [
    "# Tranforming the titles to integer representations.\n",
    "df.original_title.apply(str)\n",
    "max_len_title = train_df['original_title'].map(len).max()\n",
    "\n",
    "# train title\n",
    "\n",
    "X_title_train = tokenizer.texts_to_sequences(train_df['original_title'].values)\n",
    "X_title_train = pad_sequences(X_title_train,maxlen=max_len_title)\n",
    "# test title\n",
    "\n",
    "X_title_test = tokenizer.texts_to_sequences(test_df['original_title'].values)\n",
    "X_title_test = pad_sequences(X_title_test,maxlen=max_len_title)\n",
    "\n",
    "# val title\n",
    "X_title_val = tokenizer.texts_to_sequences(val_df['original_title'].values)\n",
    "X_title_val = pad_sequences(X_title_val,maxlen=max_len_title)\n",
    "\n",
    "print(f'Train title shape:{X_title_train.shape}, Test overview shape:{X_title_test.shape}, Validation overview shape:{X_title_val.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the dimensions of the tensors for when passing them in batches to the model.\n",
    "X_img_val = val_tensors.permute(0,3,1,2)\n",
    "\n",
    "X_img_test = test_tensors.permute(0,3,1,2)\n",
    "\n",
    "X_img_train = train_tensors.permute(0,3,1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using TensorDataset to transform the arrays into tensors for my overview title data\n",
    "\n",
    "batch_size = 150\n",
    "title_train_data = TensorDataset(torch.from_numpy(X_title_train), torch.from_numpy(train_labels))\n",
    "img_train_data = TensorDataset(X_img_train, torch.from_numpy(train_labels))\n",
    "\n",
    "\n",
    "title_val_data = TensorDataset(torch.from_numpy(X_title_val), torch.from_numpy(val_labels))\n",
    "img_val_data = TensorDataset(X_img_val, torch.from_numpy(val_labels))\n",
    "title_test_data = TensorDataset(torch.from_numpy(X_title_test), torch.from_numpy(test_labels))\n",
    "img_test_data = TensorDataset(X_img_test, torch.from_numpy(test_labels))\n",
    "\n",
    "\n",
    "\n",
    "title_train_loader = DataLoader(title_train_data, batch_size=batch_size)\n",
    "img_train_loader = DataLoader(img_train_data, batch_size=batch_size)\n",
    "\n",
    "title_val_loader = DataLoader(title_val_data, batch_size=batch_size)\n",
    "img_val_loader = DataLoader(img_val_data, batch_size=batch_size)\n",
    "\n",
    "title_test_loader = DataLoader(title_test_data, batch_size=batch_size)\n",
    "img_test_loader = DataLoader(img_test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_comp = 150#df['production_companies'].map(len).max()\n",
    "X_company_train = tokenizer.texts_to_sequences(train_df['production_companies'].values)\n",
    "X_company_train = pad_sequences(X_company_train, maxlen=max_len_comp)\n",
    "\n",
    "X_company_test = tokenizer.texts_to_sequences(test_df['production_companies'].values)\n",
    "X_company_test = pad_sequences(X_company_test, maxlen=max_len_comp)\n",
    "\n",
    "X_company_val = tokenizer.texts_to_sequences(val_df['production_companies'].values)\n",
    "X_company_val = pad_sequences(X_company_val, maxlen=max_len_comp)\n",
    "\n",
    "#print(f'Train overview shape:{X_text_train.shape}, Test overview shape:{X_text_test.shape}, Validation overview shape:{X_text_val.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using TensorDataset to transform the arrays into tensors for my overview textual data\n",
    "text_train_data = TensorDataset(torch.from_numpy(X_overview_train), torch.from_numpy(train_labels))\n",
    "img_train_data = TensorDataset(X_img_train, torch.from_numpy(train_labels))\n",
    "\n",
    "\n",
    "text_val_data = TensorDataset(torch.from_numpy(X_overview_val), torch.from_numpy(val_labels))\n",
    "img_val_data = TensorDataset(X_img_val, torch.from_numpy(val_labels))\n",
    "text_test_data = TensorDataset(torch.from_numpy(X_overview_test), torch.from_numpy(test_labels))\n",
    "img_test_data = TensorDataset(X_img_test, torch.from_numpy(test_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text_train_loader = DataLoader(text_train_data, batch_size=batch_size)\n",
    "img_train_loader = DataLoader(img_train_data, batch_size=batch_size)\n",
    "\n",
    "text_val_loader = DataLoader(text_val_data, batch_size=batch_size)\n",
    "img_val_loader = DataLoader(img_val_data, batch_size=batch_size)\n",
    "\n",
    "text_test_loader = DataLoader(text_test_data, batch_size=batch_size)\n",
    "img_test_loader = DataLoader(img_test_data, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using TensorDataset to transform the arrays into tensors for my overview textual data\n",
    "company_train_data = TensorDataset(torch.from_numpy(X_company_train), torch.from_numpy(train_labels))\n",
    "img_train_data = TensorDataset(X_img_train, torch.from_numpy(train_labels))\n",
    "\n",
    "\n",
    "company_val_data = TensorDataset(torch.from_numpy(X_company_val), torch.from_numpy(val_labels))\n",
    "img_val_data = TensorDataset(X_img_val, torch.from_numpy(val_labels))\n",
    "company_test_data = TensorDataset(torch.from_numpy(X_company_test), torch.from_numpy(test_labels))\n",
    "img_test_data = TensorDataset(X_img_test, torch.from_numpy(test_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "company_train_loader = DataLoader(company_train_data, batch_size=batch_size)\n",
    "img_train_loader = DataLoader(img_train_data, batch_size=batch_size)\n",
    "\n",
    "company_val_loader = DataLoader(company_val_data, batch_size=batch_size)\n",
    "img_val_loader = DataLoader(img_val_data, batch_size=batch_size)\n",
    "\n",
    "company_test_loader = DataLoader(company_test_data, batch_size=batch_size)\n",
    "img_test_loader = DataLoader(img_test_data, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Title and Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 150\n",
    "epochs = 20\n",
    "clip = 5\n",
    "vocab_size = len(word_index)+1\n",
    "output_size = train_labels.shape[1]\n",
    "embedding_dim = 300\n",
    "hidden_dim = 64\n",
    "n_layers = 2\n",
    "lr=0.001\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building model for movie genre classification using textual and image features.\n",
    "# CNN_LSTM2sc takes two features as input, The multimodal features can be either concatenated or summed. Change self.concat to False to sum.\n",
    "class CNN_LSTM2sc(nn.Module):\n",
    "    def __init__(self, vocab_size, weights_matrix, n_hidden, n_layers, n_out, concat=True):\n",
    "        super(CNN_LSTM2sc, self).__init__()\n",
    "\n",
    "        # LSTM for the text overview\n",
    "        self.vocab_size, self.n_hidden, self.n_out, self.n_layers = vocab_size, n_hidden, n_out, n_layers\n",
    "        num_embeddings, embedding_dim = weights_matrix.shape[0], weights_matrix.shape[1]\n",
    "        self.emb = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.emb.weight.data.copy_(torch.from_numpy(weights_matrix))\n",
    "        self.emb.weight.requires_grad = True\n",
    "        self.lstm = nn.LSTM(embedding_dim, self.n_hidden, self.n_layers, dropout=0.2, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.lstm_fc = nn.Linear(self.n_hidden, 128)\n",
    "        self.concat = concat\n",
    "        \n",
    "        # CNN for the posters\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3) # Gives me depth of input.\n",
    "        # pooling layers to reduce the image size\n",
    "        self.max_pool1 = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3)\n",
    "        self.max_pool2 = nn.MaxPool2d(2)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3)\n",
    "        self.max_pool3 = nn.MaxPool2d(2)\n",
    "        self.conv4 = nn.Conv2d(128, 128, 3)\n",
    "        self.max_pool4 = nn.MaxPool2d(2)\n",
    "        # to drop the nodes that are below the wanted weights\n",
    "        self.cnn_dropout = nn.Dropout(0.1)\n",
    "        if self.concat == True:\n",
    "            self.cnn_fc = nn.Linear(4*4*128, 512) # Gives me number of features of input.         \n",
    "            self.combined_fc1 = nn.Linear(640, 256) # Give me features of input.\n",
    "            self.combined_fc2 = nn.Linear(256, 128)\n",
    "            self.output_fc = nn.Linear(128, n_out)\n",
    "        \n",
    "        else:            \n",
    "            self.cnn_fc = nn.Linear(4*4*128, 128) # when we sum, we want our textual and visual features to be of the same size so we can stack them\n",
    "            # Concat layer for the combined feature space\n",
    "            self.combined_fc1 = nn.Linear(128, 64) # Give me features of input.\n",
    "            self.combined_fc2 = nn.Linear(64, 32)\n",
    "            self.output_fc = nn.Linear(32, n_out)\n",
    "\n",
    "\n",
    "    def forward(self, lstm_inp, cnn_inp):\n",
    "        batch_size = lstm_inp.size(0)\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        lstm_inp = lstm_inp.long()\n",
    "        embeds = self.emb(lstm_inp)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        lstm_out = self.dropout(lstm_out[:, -1])\n",
    "        lstm_out = F.relu(self.lstm_fc(lstm_out))\n",
    "        \n",
    "        x = F.relu(self.conv1(cnn_inp))\n",
    "        x = self.max_pool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.max_pool2(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.max_pool3(x)\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.max_pool4(x)\n",
    "        # that is how I get the correct dimensions for the CNN output that is then passed as input to the linear layer\n",
    "        # need the input to be two dimensional again\n",
    "        x = x.view(-1, 4*4*128)\n",
    "        x = self.cnn_dropout(x)\n",
    "        \n",
    "        if self.concat == True:\n",
    "            cnn_out = F.relu(self.cnn_fc(x))            \n",
    "            #concatenating the cnn output and the lstm output\n",
    "            combined_inp = torch.cat((cnn_out, lstm_out), 1)\n",
    "            # activation function\n",
    "            x_comb = F.relu(self.combined_fc1(combined_inp))\n",
    "            x_comb = F.relu(self.combined_fc2(x_comb))\n",
    "            # sigmoid to get probabilities for each label\n",
    "            out = torch.sigmoid(self.output_fc(x_comb))\n",
    "        else: # sum happens here\n",
    "            cnn_out = F.relu(self.cnn_fc(x))            \n",
    "            combined_inp = torch.stack([cnn_out, lstm_out], 1)            \n",
    "            sum_comb = torch.sum(combined_inp,dim=1)           \n",
    "            # activation function\n",
    "            x_comb = F.relu(self.combined_fc1(sum_comb))\n",
    "            x_comb = F.relu(self.combined_fc2(x_comb))\n",
    "            # sigmoid to get probabilities for each label\n",
    "            out = torch.sigmoid(self.output_fc(x_comb))     \n",
    "                              \n",
    "        return out\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device),\n",
    "                          weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device))\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building model for movie genre classification using textual and image features.\n",
    "# CNN_LSTM3sc takes three features as input. The multimodal features can be either concatenated or summed. Change self.concat to False to sum.\n",
    "class CNN_LSTM3sc(nn.Module):\n",
    "    def __init__(self, vocab_size, weights_matrix, n_hidden, n_layers, n_out, concat=True):\n",
    "        super(CNN_LSTM3sc, self).__init__()\n",
    "\n",
    "        # LSTM for the text overview\n",
    "        self.vocab_size, self.n_hidden, self.n_out, self.n_layers = vocab_size, n_hidden, n_out, n_layers\n",
    "        num_embeddings, embedding_dim = weights_matrix.shape[0], weights_matrix.shape[1]\n",
    "        self.emb = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.emb.weight.data.copy_(torch.from_numpy(weights_matrix))\n",
    "        self.emb.weight.requires_grad = True\n",
    "        self.lstm = nn.LSTM(embedding_dim, self.n_hidden, self.n_layers, dropout=0.2, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.lstm_fc = nn.Linear(self.n_hidden, 128)\n",
    "        self.concat = concat\n",
    "        \n",
    "        # CNN for the posters\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3) # Gives me depth of input.\n",
    "        # pooling layers to reduce the image size\n",
    "        self.max_pool1 = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3)\n",
    "        self.max_pool2 = nn.MaxPool2d(2)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3)\n",
    "        self.max_pool3 = nn.MaxPool2d(2)\n",
    "        self.conv4 = nn.Conv2d(128, 128, 3)\n",
    "        self.max_pool4 = nn.MaxPool2d(2)\n",
    "        # to drop the nodes that are below the wanted weights\n",
    "        self.cnn_dropout = nn.Dropout(0.1)\n",
    "        if self.concat == True:\n",
    "            self.cnn_fc = nn.Linear(4*4*128, 512) # Gives me number of features of input.\n",
    "        # Concat layer for the combined feature space\n",
    "            self.combined_fc1 = nn.Linear(640, 256) # Give me features of input.\n",
    "            self.combined_fc2 = nn.Linear(256, 128)\n",
    "            self.output_fc = nn.Linear(128, n_out)        \n",
    "        else:            \n",
    "            self.cnn_fc = nn.Linear(4*4*128, 128) # when we sum, we want our textual and visual features to be of the same size so we can stack them\n",
    "            # Concat layer for the combined feature space\n",
    "            self.combined_fc1 = nn.Linear(128, 64) # Give me features of input.\n",
    "            self.combined_fc2 = nn.Linear(64, 32)\n",
    "            self.output_fc = nn.Linear(32, n_out)\n",
    "\n",
    "    def forward(self, lstm_in, cnn_inp, title_inp):\n",
    "        batch_size = lstm_in.size(0)\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        lstm_in = lstm_in.long()\n",
    "        title_inp = title_inp.long()\n",
    "        embeds_lstm = self.emb(lstm_in)\n",
    "        embeds_title = self.emb(title_inp)\n",
    "        embeds = torch.cat((embeds_lstm,embeds_title),dim=1)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        lstm_out = self.dropout(lstm_out[:, -1])\n",
    "        lstm_out = F.relu(self.lstm_fc(lstm_out))\n",
    "       \n",
    "        x = F.relu(self.conv1(cnn_inp))\n",
    "        x = self.max_pool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.max_pool2(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.max_pool3(x)\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.max_pool4(x)\n",
    "        # need the input to be two dimensional again\n",
    "        x = x.view(-1, 4*4*128)        \n",
    "        x = self.cnn_dropout(x)        \n",
    "        if self.concat == True: # concatenate the multimodal features\n",
    "            cnn_out = F.relu(self.cnn_fc(x))            \n",
    "            #concatenating the cnn output and the lstm output\n",
    "            combined_inp = torch.cat((cnn_out, lstm_out), 1)\n",
    "            # activation function\n",
    "            x_comb = F.relu(self.combined_fc1(combined_inp))\n",
    "            x_comb = F.relu(self.combined_fc2(x_comb))\n",
    "            # sigmoid to get probabilities for each label\n",
    "            out = torch.sigmoid(self.output_fc(x_comb))\n",
    "        else: # sum the multimodal features\n",
    "            cnn_out = F.relu(self.cnn_fc(x))\n",
    "            # sum happens here\n",
    "            combined_inp = torch.stack([cnn_out, lstm_out], 1)            \n",
    "            sum_comb = torch.sum(combined_inp,dim=1)\n",
    "            # activation function\n",
    "            x_comb = F.relu(self.combined_fc1(sum_comb))\n",
    "            x_comb = F.relu(self.combined_fc2(x_comb))\n",
    "            # # sigmoid to get probabilities for each label\n",
    "            out = torch.sigmoid(self.output_fc(x_comb))     \n",
    "                              \n",
    "        return out\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device),\n",
    "                          weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device))\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building model for movie genre classification using textual and image features.\n",
    "# CNN_LSTM4sc takes four features as input. The multimodal features can be either concatenated or summed. Change self.concat to False to sum.\n",
    "class CNN_LSTM4sc(nn.Module):\n",
    "    def __init__(self, vocab_size, weights_matrix, n_hidden, n_layers, n_out, concat=True):\n",
    "        super(CNN_LSTM4sc, self).__init__()\n",
    "\n",
    "        # LSTM for the text overview\n",
    "        self.vocab_size, self.n_hidden, self.n_out, self.n_layers = vocab_size, n_hidden, n_out, n_layers\n",
    "        num_embeddings, embedding_dim = weights_matrix.shape[0], weights_matrix.shape[1]\n",
    "        self.emb = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.emb.weight.data.copy_(torch.from_numpy(weights_matrix))\n",
    "        self.emb.weight.requires_grad = True\n",
    "        self.lstm = nn.LSTM(embedding_dim, self.n_hidden, self.n_layers, dropout=0.2, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.lstm_fc = nn.Linear(self.n_hidden, 128)\n",
    "        self.concat = concat\n",
    "        \n",
    "        # CNN for the posters\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3) # Gives me depth of input.\n",
    "        # pooling layers to reduce the image size\n",
    "        self.max_pool1 = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3)\n",
    "        self.max_pool2 = nn.MaxPool2d(2)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3)\n",
    "        self.max_pool3 = nn.MaxPool2d(2)\n",
    "        self.conv4 = nn.Conv2d(128, 128, 3)\n",
    "        self.max_pool4 = nn.MaxPool2d(2)\n",
    "        # to drop the nodes that are below the wanted weights\n",
    "        self.cnn_dropout = nn.Dropout(0.1)\n",
    "        if self.concat == True: # concatenate the multimodal features\n",
    "            self.cnn_fc = nn.Linear(4*4*128, 512) # Gives me number of features of input.  \n",
    "            # Concat layer for the combined feature space\n",
    "            self.combined_fc1 = nn.Linear(640, 256) # Give me features of input.\n",
    "            self.combined_fc2 = nn.Linear(256, 128)\n",
    "            self.output_fc = nn.Linear(128, n_out)        \n",
    "        else:   # sum the multimodal features\n",
    "            self.cnn_fc = nn.Linear(4*4*128, 128) # when we sum, we want our textual and visual features to be of the same size so we can stack them\n",
    "            # Concat layer for the combined feature space\n",
    "            self.combined_fc1 = nn.Linear(128, 64) # Give me features of input.\n",
    "            self.combined_fc2 = nn.Linear(64, 32)\n",
    "            self.output_fc = nn.Linear(32, n_out)\n",
    "\n",
    "\n",
    "    def forward(self, lstm_in, cnn_inp, title_inp, company_inp):\n",
    "        batch_size = lstm_in.size(0)\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        lstm_in, title_inp,company_inp = lstm_in.long(), title_inp.long(), company_inp.long()\n",
    "        embeds_lstm = self.emb(lstm_in)\n",
    "        embeds_title = self.emb(title_inp)\n",
    "        embeds_company = self.emb(company_inp)\n",
    "        embeds = torch.cat((embeds_lstm,embeds_title,embeds_company),dim=1)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        lstm_out = self.dropout(lstm_out[:, -1])\n",
    "        lstm_out = F.relu(self.lstm_fc(lstm_out))\n",
    "\n",
    "        x = F.relu(self.conv1(cnn_inp))\n",
    "        x = self.max_pool1(x)\n",
    "        \n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.max_pool2(x)\n",
    "        \n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.max_pool3(x)\n",
    "        \n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.max_pool4(x)\n",
    "        # that is how I get the correct dimensions for the CNN output that is then passed as input to the linear layer\n",
    "        # need the input to be two dimensional again\n",
    "        x = x.view(-1, 4*4*128)\n",
    "        \n",
    "        x = self.cnn_dropout(x)\n",
    "        if self.concat == True: # concatenate the multimodal features\n",
    "            cnn_out = F.relu(self.cnn_fc(x))            \n",
    "            #concatenating the cnn output and the lstm output\n",
    "            combined_inp = torch.cat((cnn_out, lstm_out), 1)\n",
    "            # activation function\n",
    "            x_comb = F.relu(self.combined_fc1(combined_inp))\n",
    "            x_comb = F.relu(self.combined_fc2(x_comb))\n",
    "            # sigmoid to get probabilities for each label\n",
    "            out = torch.sigmoid(self.output_fc(x_comb))\n",
    "        else: # sum the multimodal features\n",
    "            cnn_out = F.relu(self.cnn_fc(x))\n",
    "            combined_inp = torch.stack([cnn_out, lstm_out], 1)            \n",
    "            sum_comb = torch.sum(combined_inp,dim=1)\n",
    "            # activation function\n",
    "            x_comb = F.relu(self.combined_fc1(sum_comb))\n",
    "            x_comb = F.relu(self.combined_fc2(x_comb))\n",
    "            # sigmoid to get 1 and 0 on the one-hot-encoding formatted label\n",
    "            out = torch.sigmoid(self.output_fc(x_comb))     \n",
    "                              \n",
    "        return out\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device),\n",
    "                          weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device))\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done training tensor(0.8041, device='cuda:2')\n",
      "Saving model...\n",
      "Epoch 1: train_loss: 0.3321 train_acc: 0.8041 | val_loss: 0.3092 val_acc: 0.8385\n",
      "done training tensor(0.8213, device='cuda:2')\n",
      "Saving model...\n",
      "Epoch 2: train_loss: 0.3056 train_acc: 0.8213 | val_loss: 0.2995 val_acc: 0.8239\n",
      "done training tensor(0.8237, device='cuda:2')\n",
      "Saving model...\n",
      "Epoch 3: train_loss: 0.3014 train_acc: 0.8237 | val_loss: 0.2958 val_acc: 0.8290\n",
      "done training tensor(0.8262, device='cuda:2')\n",
      "Saving model...\n",
      "Epoch 4: train_loss: 0.2972 train_acc: 0.8262 | val_loss: 0.2929 val_acc: 0.8310\n",
      "done training tensor(0.8273, device='cuda:2')\n",
      "Saving model...\n",
      "Epoch 5: train_loss: 0.2952 train_acc: 0.8273 | val_loss: 0.2949 val_acc: 0.8343\n",
      "done training tensor(0.8285, device='cuda:2')\n",
      "Saving model...\n",
      "Epoch 6: train_loss: 0.2931 train_acc: 0.8285 | val_loss: 0.2934 val_acc: 0.8339\n",
      "done training tensor(0.8303, device='cuda:2')\n",
      "Saving model...\n",
      "Epoch 7: train_loss: 0.2898 train_acc: 0.8303 | val_loss: 0.2916 val_acc: 0.8359\n",
      "done training tensor(0.8319, device='cuda:2')\n",
      "Saving model...\n",
      "Epoch 8: train_loss: 0.2868 train_acc: 0.8319 | val_loss: 0.2887 val_acc: 0.8337\n",
      "done training tensor(0.8339, device='cuda:2')\n",
      "Saving model...\n",
      "Epoch 9: train_loss: 0.2831 train_acc: 0.8339 | val_loss: 0.2885 val_acc: 0.8353\n",
      "done training tensor(0.8362, device='cuda:2')\n",
      "Saving model...\n",
      "Epoch 10: train_loss: 0.2787 train_acc: 0.8362 | val_loss: 0.2942 val_acc: 0.8405\n",
      "done training tensor(0.8389, device='cuda:2')\n",
      "Saving model...\n",
      "Epoch 11: train_loss: 0.2740 train_acc: 0.8389 | val_loss: 0.2965 val_acc: 0.8391\n",
      "done training tensor(0.8414, device='cuda:2')\n",
      "Saving model...\n",
      "Epoch 12: train_loss: 0.2695 train_acc: 0.8414 | val_loss: 0.2990 val_acc: 0.8418\n",
      "done training tensor(0.8434, device='cuda:2')\n",
      "Saving model...\n",
      "Epoch 13: train_loss: 0.2661 train_acc: 0.8434 | val_loss: 0.3003 val_acc: 0.8369\n",
      "done training tensor(0.8460, device='cuda:2')\n",
      "Saving model...\n",
      "Epoch 14: train_loss: 0.2616 train_acc: 0.8460 | val_loss: 0.3052 val_acc: 0.8364\n",
      "done training tensor(0.8477, device='cuda:2')\n",
      "Saving model...\n",
      "Epoch 15: train_loss: 0.2588 train_acc: 0.8477 | val_loss: 0.3124 val_acc: 0.8381\n",
      "done training tensor(0.8505, device='cuda:2')\n",
      "Saving model...\n",
      "Epoch 16: train_loss: 0.2539 train_acc: 0.8505 | val_loss: 0.3143 val_acc: 0.8300\n",
      "done training tensor(0.8540, device='cuda:2')\n",
      "Saving model...\n",
      "Epoch 17: train_loss: 0.2476 train_acc: 0.8540 | val_loss: 0.3193 val_acc: 0.8356\n",
      "done training tensor(0.8568, device='cuda:2')\n",
      "Saving model...\n",
      "Epoch 18: train_loss: 0.2430 train_acc: 0.8568 | val_loss: 0.3251 val_acc: 0.8350\n",
      "done training tensor(0.8605, device='cuda:2')\n",
      "Saving model...\n",
      "Epoch 19: train_loss: 0.2364 train_acc: 0.8605 | val_loss: 0.3297 val_acc: 0.8365\n",
      "done training tensor(0.8635, device='cuda:2')\n",
      "Saving model...\n",
      "Epoch 20: train_loss: 0.2311 train_acc: 0.8635 | val_loss: 0.3468 val_acc: 0.8369\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:2')\n",
    "model_title_sum = CNN_LSTM2sc(vocab_size, word2vec_matrix, hidden_dim, n_layers, output_size,False)\n",
    "model_title_sum.to(device)\n",
    "train_val(model_title_sum,epochs, title_train_loader, img_train_loader,criterion, title_val_loader, img_val_loader, 2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN_LSTM2sc(\n",
       "  (emb): Embedding(70174, 300)\n",
       "  (lstm): LSTM(300, 64, num_layers=2, batch_first=True, dropout=0.2)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (lstm_fc): Linear(in_features=64, out_features=128, bias=True)\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (max_pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (max_pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (max_pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (max_pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (cnn_dropout): Dropout(p=0.1, inplace=False)\n",
       "  (cnn_fc): Linear(in_features=2048, out_features=512, bias=True)\n",
       "  (combined_fc1): Linear(in_features=640, out_features=256, bias=True)\n",
       "  (combined_fc2): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (output_fc): Linear(in_features=128, out_features=19, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_title = CNN_LSTM2sc(vocab_size, word2vec_matrix, hidden_dim, n_layers, output_size)\n",
    "model_title.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_feats can be either 2,3 or 4, i.e. the number of different features (overview, title,\n",
    "# companies, poster are the possible features) used as input to the model.\n",
    "# model_num is just a random integer to save the models we train.\n",
    "def train_val(model,epochs, t_train_loader, image_train_loader, criterion, t_val_loader, image_val_loader, num_feats, model_num):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    model.train()\n",
    "    for i in range(epochs):\n",
    "        total_acc_train = 0\n",
    "        total_loss_train = 0        \n",
    "        for lstm, cnn in zip(t_train_loader, image_train_loader):\n",
    "            cnn_inp, cnn_labels = cnn\n",
    "            cnn_inp = cnn_inp.float()\n",
    "            cnn_inp, cnn_labels = cnn_inp.to(device), cnn_labels.to(device)\n",
    "            if num_feats == 2:\n",
    "                lstm_inp,lstm_labels = lstm\n",
    "                lstm_inp = lstm_inp.float()                \n",
    "                lstm_inp, lstm_labels = lstm_inp.to(device), lstm_labels.to(device)                \n",
    "                model.zero_grad()\n",
    "                output = model(lstm_inp, cnn_inp)                \n",
    "            elif num_feats == 3:\n",
    "                lstm_inp, title_inp, lstm_labels = lstm        \n",
    "                title_inp = title_inp.float()\n",
    "                lstm_inp = lstm_inp.float()               \n",
    "                lstm_inp = lstm_inp.to(device)\n",
    "                title_inp = title_inp.to(device)\n",
    "                lstm_labels =  lstm_labels.to(device)      \n",
    "                model.zero_grad()\n",
    "                output = model(lstm_inp, cnn_inp, title_inp)                \n",
    "            else:\n",
    "                lstm_inp, title_inp, company_inp, lstm_labels = lstm\n",
    "                lstm_inp = lstm_inp.float()\n",
    "                title_inp = title_inp.float()\n",
    "                company_inp = company_inp.float()                \n",
    "                lstm_inp, title_inp, company_inp,lstm_labels = lstm_inp.to(device),title_inp.to(device), company_inp.to(device), lstm_labels.to(device)\n",
    "                \n",
    "                model.zero_grad()\n",
    "                output = model(lstm_inp, cnn_inp, title_inp,company_inp)\n",
    "                \n",
    "            loss = criterion(output.squeeze(), lstm_labels.float())\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            optimizer.step()\n",
    "        \n",
    "            with torch.no_grad():\n",
    "                acc = torch.abs(output.squeeze() - lstm_labels.float()).view(-1)\n",
    "                acc = (1. - acc.sum() / acc.size()[0])\n",
    "                total_acc_train += acc\n",
    "                total_loss_train += loss.item()\n",
    "      \n",
    "        train_acc = total_acc_train/len(t_train_loader)\n",
    "        train_loss = total_loss_train/len(t_train_loader)\n",
    "        print('done training',train_acc)\n",
    "        model.eval()\n",
    "        total_acc_val = 0\n",
    "        total_loss_val = 0\n",
    "        with torch.no_grad():\n",
    "            for lstm, cnn in zip(t_val_loader, image_val_loader):\n",
    "                cnn_inp, cnn_labels = cnn\n",
    "                cnn_inp = cnn_inp.float()\n",
    "                cnn_inp, cnn_labels = cnn_inp.to(device), cnn_labels.to(device)\n",
    "                if num_feats == 2:\n",
    "                    lstm_inp,lstm_labels = lstm\n",
    "                    lstm_inp = lstm_inp.float()                \n",
    "                    lstm_inp, lstm_labels = lstm_inp.to(device), lstm_labels.to(device)                \n",
    "                    model.zero_grad()\n",
    "                    output = model(lstm_inp, cnn_inp) \n",
    "                elif num_feats == 3:\n",
    "                    lstm_inp, title_inp, lstm_labels = lstm        \n",
    "                    title_inp = title_inp.float()\n",
    "                    lstm_inp = lstm_inp.float()               \n",
    "                    lstm_inp = lstm_inp.to(device)\n",
    "                    title_inp = title_inp.to(device)\n",
    "                    lstm_labels =  lstm_labels.to(device)      \n",
    "                    model.zero_grad()\n",
    "                    output = model(lstm_inp, cnn_inp,title_inp)\n",
    "                else:\n",
    "                    lstm_inp, title_inp, company_inp, lstm_labels = lstm\n",
    "                    lstm_inp = lstm_inp.float()\n",
    "                    title_inp = title_inp.float()\n",
    "                    company_inp = company_inp.float()                \n",
    "                    lstm_inp, title_inp, company_inp,lstm_labels = lstm_inp.to(device),title_inp.to(device), company_inp.to(device), lstm_labels.to(device)\n",
    "                    model.zero_grad()\n",
    "                    output = model(lstm_inp, cnn_inp, title_inp,company_inp)\n",
    "                    \n",
    "                    \n",
    "                val_loss = criterion(output.squeeze(), lstm_labels.float())\n",
    "                acc = torch.abs(output.squeeze() - lstm_labels.float()).view(-1)\n",
    "                acc = (1. - acc.sum() / acc.size()[0])\n",
    "                total_acc_val += acc\n",
    "                total_loss_val += val_loss.item()\n",
    "        print(\"Saving model...\") \n",
    "        # 10 diff models to save\n",
    "        if model_num == 0:\n",
    "            torch.save(model_title.state_dict(), '../data/models/model0.pt')\n",
    "        elif model_num == 1:\n",
    "            torch.save(model_title.state_dict(), '../data/models/model1.pt')    \n",
    "        elif model_num == 2:\n",
    "            torch.save(model_title.state_dict(), '../data/models/model2.pt')\n",
    "        elif model_num == 3:\n",
    "            torch.save(model_title.state_dict(), '../data/models/model3.pt')\n",
    "        elif model_num == 4:\n",
    "            torch.save(model_title.state_dict(), '../data/models/model4.pt')\n",
    "        elif model_num == 5:\n",
    "            torch.save(model_title.state_dict(), '../data/models/model5.pt')\n",
    "        elif model_num == 6:\n",
    "            torch.save(model_title.state_dict(), '../data/models/model6.pt')\n",
    "        elif model_num == 7:\n",
    "            torch.save(model_title.state_dict(), '../data/models/model7.pt')\n",
    "        elif model_num == 8:\n",
    "            torch.save(model_title.state_dict(), '../data/models/model8.pt')\n",
    "        elif model_num == 9:\n",
    "            torch.save(model_title.state_dict(), '../data/models/model9.pt')\n",
    "    \n",
    "        val_acc = total_acc_val/len(t_val_loader)\n",
    "        val_loss = total_loss_val/len(t_val_loader)\n",
    "        print(f'Epoch {i+1}: train_loss: {train_loss:.4f} train_acc: {train_acc:.4f} | val_loss: {val_loss:.4f} val_acc: {val_acc:.4f}')\n",
    "        model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done training tensor(0.8140, device='cuda:1')\n",
      "Saving model_title...\n",
      "Epoch 1: train_loss: 0.3167 train_acc: 0.8140 | val_loss: 0.3006 val_acc: 0.8334\n",
      "done training tensor(0.8235, device='cuda:1')\n",
      "Saving model_title...\n",
      "Epoch 2: train_loss: 0.3018 train_acc: 0.8235 | val_loss: 0.2957 val_acc: 0.8315\n",
      "done training tensor(0.8258, device='cuda:1')\n",
      "Saving model_title...\n",
      "Epoch 3: train_loss: 0.2980 train_acc: 0.8258 | val_loss: 0.2941 val_acc: 0.8351\n",
      "done training tensor(0.8282, device='cuda:1')\n",
      "Saving model_title...\n",
      "Epoch 4: train_loss: 0.2932 train_acc: 0.8282 | val_loss: 0.2898 val_acc: 0.8334\n",
      "done training tensor(0.8317, device='cuda:1')\n",
      "Saving model_title...\n",
      "Epoch 5: train_loss: 0.2870 train_acc: 0.8317 | val_loss: 0.2902 val_acc: 0.8369\n",
      "done training tensor(0.8351, device='cuda:1')\n",
      "Saving model_title...\n",
      "Epoch 6: train_loss: 0.2809 train_acc: 0.8351 | val_loss: 0.2872 val_acc: 0.8337\n",
      "done training tensor(0.8385, device='cuda:1')\n",
      "Saving model_title...\n",
      "Epoch 7: train_loss: 0.2750 train_acc: 0.8385 | val_loss: 0.2925 val_acc: 0.8398\n",
      "done training tensor(0.8422, device='cuda:1')\n",
      "Saving model_title...\n",
      "Epoch 8: train_loss: 0.2684 train_acc: 0.8422 | val_loss: 0.2984 val_acc: 0.8426\n",
      "done training tensor(0.8448, device='cuda:1')\n",
      "Saving model_title...\n",
      "Epoch 9: train_loss: 0.2636 train_acc: 0.8448 | val_loss: 0.2951 val_acc: 0.8422\n",
      "done training tensor(0.8480, device='cuda:1')\n",
      "Saving model_title...\n",
      "Epoch 10: train_loss: 0.2584 train_acc: 0.8480 | val_loss: 0.2955 val_acc: 0.8393\n",
      "done training tensor(0.8505, device='cuda:1')\n",
      "Saving model_title...\n",
      "Epoch 11: train_loss: 0.2540 train_acc: 0.8505 | val_loss: 0.3011 val_acc: 0.8418\n",
      "done training tensor(0.8546, device='cuda:1')\n",
      "Saving model_title...\n",
      "Epoch 12: train_loss: 0.2469 train_acc: 0.8546 | val_loss: 0.3096 val_acc: 0.8447\n",
      "done training tensor(0.8578, device='cuda:1')\n",
      "Saving model_title...\n",
      "Epoch 13: train_loss: 0.2413 train_acc: 0.8578 | val_loss: 0.3155 val_acc: 0.8437\n",
      "done training tensor(0.8606, device='cuda:1')\n",
      "Saving model_title...\n",
      "Epoch 14: train_loss: 0.2366 train_acc: 0.8606 | val_loss: 0.3249 val_acc: 0.8371\n",
      "done training tensor(0.8645, device='cuda:1')\n",
      "Saving model_title...\n",
      "Epoch 15: train_loss: 0.2299 train_acc: 0.8645 | val_loss: 0.3272 val_acc: 0.8338\n",
      "done training tensor(0.8698, device='cuda:1')\n",
      "Saving model_title...\n",
      "Epoch 16: train_loss: 0.2207 train_acc: 0.8698 | val_loss: 0.3327 val_acc: 0.8377\n",
      "done training tensor(0.8745, device='cuda:1')\n",
      "Saving model_title...\n",
      "Epoch 17: train_loss: 0.2127 train_acc: 0.8745 | val_loss: 0.3382 val_acc: 0.8411\n",
      "done training tensor(0.8793, device='cuda:1')\n",
      "Saving model_title...\n",
      "Epoch 18: train_loss: 0.2044 train_acc: 0.8793 | val_loss: 0.3587 val_acc: 0.8438\n",
      "done training tensor(0.8838, device='cuda:1')\n",
      "Saving model_title...\n",
      "Epoch 19: train_loss: 0.1969 train_acc: 0.8838 | val_loss: 0.3503 val_acc: 0.8475\n",
      "done training tensor(0.8879, device='cuda:1')\n",
      "Saving model_title...\n",
      "Epoch 20: train_loss: 0.1900 train_acc: 0.8879 | val_loss: 0.3608 val_acc: 0.8465\n"
     ]
    }
   ],
   "source": [
    "train_val(model_title,epochs, title_train_loader, img_train_loader,criterion, title_val_loader, img_val_loader, 2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function returns accuracy and roc auc score of a model on the test set.\n",
    "def test_acc_auc(model,text_test_loader, img_test_loader, criterion,num_feats):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_acc_test = 0\n",
    "    total_loss_test = 0\n",
    "    outputs = []\n",
    "    with torch.no_grad():\n",
    "        for lstm, cnn in zip(text_test_loader, img_test_loader):\n",
    "            cnn_inp, cnn_labels = cnn\n",
    "            cnn_inp = cnn_inp.float()\n",
    "            cnn_inp, cnn_labels = cnn_inp.to(device), cnn_labels.to(device)\n",
    "            if num_feats == 2:\n",
    "                lstm_inp, lstm_labels = lstm\n",
    "                lstm_inp = lstm_inp.float()\n",
    "                \n",
    "                lstm_inp, lstm_labels = lstm_inp.to(device), lstm_labels.to(device)\n",
    "                \n",
    "                out = model(lstm_inp, cnn_inp)\n",
    "            elif num_feats == 3:\n",
    "                lstm_inp, title_inp, lstm_labels = lstm\n",
    "                lstm_inp = lstm_inp.float()\n",
    "                title_inp = title_inp.float()\n",
    "                lstm_inp, title_inp,lstm_labels = lstm_inp.to(device),title_inp.to(device), lstm_labels.to(device)\n",
    "                out = model(lstm_inp, cnn_inp,title_inp)\n",
    "            else:\n",
    "                lstm_inp, title_inp, company_inp, lstm_labels = lstm\n",
    "                lstm_inp = lstm_inp.float()\n",
    "                title_inp = title_inp.float()\n",
    "                company_inp = company_inp.float()                \n",
    "                lstm_inp, title_inp, company_inp, lstm_labels = lstm_inp.to(device), title_inp.to(device), company_inp.to(device),  lstm_labels.to(device)                \n",
    "                out = model(lstm_inp, cnn_inp, title_inp, company_inp)\n",
    "                \n",
    "            \n",
    "                  \n",
    "            outputs += list(out.cpu().data.numpy())\n",
    "            loss = criterion(out.squeeze(), lstm_labels.float())\n",
    "            acc = torch.abs(out.squeeze() - lstm_labels.float()).view(-1)\n",
    "            acc = (1. - acc.sum() / acc.size()[0])\n",
    "            total_acc_test += acc\n",
    "            total_loss_test += loss.item()\n",
    "    \n",
    "    acc_test = total_acc_test/len(title_test_loader)\n",
    "    loss_test = total_loss_test/len(title_test_loader)        \n",
    "    \n",
    "    np_out = np.array(outputs)\n",
    "    y_pred = np.zeros(np_out.shape)\n",
    "    y_pred[np_out>0.5]= 1 # threshold to assign a label is a probability of 0.5.\n",
    "    y_pred = np.array(y_pred)    \n",
    "    preds = np.transpose(y_pred)\n",
    "    labels = np.transpose(test_labels)\n",
    "    roc_auc = roc_auc_score(labels, preds) # calculating ROC-AUC score\n",
    "    print(f'acc: {acc_test:.4f}, loss: {loss_test:.4f}, roc auc: {roc_auc}')\n",
    "    return acc_test, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.8462 loss: 0.3624 roc auc: 0.6462277587954535\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.8462, device='cuda:1'), 0.6462277587954535)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc_auc(model_title, title_test_loader, img_test_loader, criterion,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview and Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN_LSTM2sc(\n",
       "  (emb): Embedding(70174, 300)\n",
       "  (lstm): LSTM(300, 64, num_layers=2, batch_first=True, dropout=0.2)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (lstm_fc): Linear(in_features=64, out_features=128, bias=True)\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (max_pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (max_pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (max_pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (max_pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (cnn_dropout): Dropout(p=0.1, inplace=False)\n",
       "  (cnn_fc): Linear(in_features=2048, out_features=512, bias=True)\n",
       "  (combined_fc1): Linear(in_features=640, out_features=256, bias=True)\n",
       "  (combined_fc2): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (output_fc): Linear(in_features=128, out_features=19, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_overview = CNN_LSTM2sc(vocab_size, word2vec_matrix, hidden_dim, n_layers, output_size)\n",
    "model_overview.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val(model_overview, epochs, text_train_loader, img_train_loader,criterion, text_val_loader, img_val_loader, 2,2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt at using title, image, poster as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempting to add title when loading the dataset.\n",
    "# Again, using TensorDataset to transform the arrays into tensors for my textual data.\n",
    "\n",
    "# train dataset\n",
    "text_train_data = TensorDataset(torch.from_numpy(X_overview_train), torch.from_numpy(X_title_train), torch.from_numpy(train_labels))\n",
    "img_train_data = TensorDataset(X_img_train, torch.from_numpy(train_labels))\n",
    "\n",
    "# validation dataset\n",
    "text_val_data = TensorDataset(torch.from_numpy(X_overview_val), torch.from_numpy(X_title_val), torch.from_numpy(val_labels))\n",
    "img_val_data = TensorDataset(X_img_val, torch.from_numpy(val_labels))\n",
    "\n",
    "# test dataset\n",
    "text_test_data = TensorDataset(torch.from_numpy(X_overview_test),torch.from_numpy(X_title_test), torch.from_numpy(test_labels))\n",
    "img_test_data = TensorDataset(X_img_test, torch.from_numpy(test_labels))\n",
    "\n",
    "batch_size = 150\n",
    "\n",
    "text_train_loader = DataLoader(text_train_data, batch_size=batch_size)\n",
    "img_train_loader = DataLoader(img_train_data, batch_size=batch_size)\n",
    "\n",
    "text_val_loader = DataLoader(text_val_data, batch_size=batch_size)\n",
    "img_val_loader = DataLoader(img_val_data, batch_size=batch_size)\n",
    "\n",
    "text_test_loader = DataLoader(text_test_data, batch_size=batch_size)\n",
    "img_test_loader = DataLoader(img_test_data, batch_size=batch_size)\n",
    " #using TensorDataset to transform the arrays into tensors for my overview textual data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN_LSTM3(\n",
       "  (emb): Embedding(70174, 300)\n",
       "  (lstm): LSTM(300, 64, num_layers=2, batch_first=True, dropout=0.2)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (lstm_fc): Linear(in_features=64, out_features=128, bias=True)\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (max_pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (max_pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (max_pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (max_pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (cnn_dropout): Dropout(p=0.1, inplace=False)\n",
       "  (cnn_fc): Linear(in_features=2048, out_features=512, bias=True)\n",
       "  (combined_fc1): Linear(in_features=640, out_features=256, bias=True)\n",
       "  (combined_fc2): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (output_fc): Linear(in_features=128, out_features=19, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3 = CNN_LSTM3sc(vocab_size, word2vec_matrix, hidden_dim, n_layers, output_size)\n",
    "model3.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Companies and Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done training tensor(0.8145, device='cuda:1')\n",
      "Saving model...\n",
      "Epoch 1: train_loss: 0.3171 train_acc: 0.8145 | val_loss: 0.3022 val_acc: 0.8325\n",
      "done training tensor(0.8236, device='cuda:1')\n",
      "Saving model...\n",
      "Epoch 2: train_loss: 0.3015 train_acc: 0.8236 | val_loss: 0.2980 val_acc: 0.8391\n",
      "done training tensor(0.8261, device='cuda:1')\n",
      "Saving model...\n",
      "Epoch 3: train_loss: 0.2970 train_acc: 0.8261 | val_loss: 0.2909 val_acc: 0.8326\n",
      "done training tensor(0.8299, device='cuda:1')\n",
      "Saving model...\n",
      "Epoch 4: train_loss: 0.2899 train_acc: 0.8299 | val_loss: 0.2877 val_acc: 0.8357\n",
      "done training tensor(0.8338, device='cuda:1')\n",
      "Saving model...\n",
      "Epoch 5: train_loss: 0.2832 train_acc: 0.8338 | val_loss: 0.2836 val_acc: 0.8372\n",
      "done training tensor(0.8371, device='cuda:1')\n",
      "Saving model...\n",
      "Epoch 6: train_loss: 0.2770 train_acc: 0.8371 | val_loss: 0.2831 val_acc: 0.8406\n",
      "done training tensor(0.8402, device='cuda:1')\n",
      "Saving model...\n",
      "Epoch 7: train_loss: 0.2714 train_acc: 0.8402 | val_loss: 0.2853 val_acc: 0.8442\n",
      "done training tensor(0.8433, device='cuda:1')\n",
      "Saving model...\n",
      "Epoch 8: train_loss: 0.2657 train_acc: 0.8433 | val_loss: 0.2919 val_acc: 0.8502\n",
      "done training tensor(0.8462, device='cuda:1')\n",
      "Saving model...\n",
      "Epoch 9: train_loss: 0.2602 train_acc: 0.8462 | val_loss: 0.2904 val_acc: 0.8456\n",
      "done training tensor(0.8479, device='cuda:1')\n",
      "Saving model...\n",
      "Epoch 10: train_loss: 0.2573 train_acc: 0.8479 | val_loss: 0.2931 val_acc: 0.8413\n",
      "done training tensor(0.8510, device='cuda:1')\n",
      "Saving model...\n",
      "Epoch 11: train_loss: 0.2515 train_acc: 0.8510 | val_loss: 0.2986 val_acc: 0.8434\n",
      "done training tensor(0.8554, device='cuda:1')\n",
      "Saving model...\n",
      "Epoch 12: train_loss: 0.2438 train_acc: 0.8554 | val_loss: 0.2977 val_acc: 0.8359\n",
      "done training tensor(0.8593, device='cuda:1')\n",
      "Saving model...\n",
      "Epoch 13: train_loss: 0.2371 train_acc: 0.8593 | val_loss: 0.3098 val_acc: 0.8297\n",
      "done training tensor(0.8618, device='cuda:1')\n",
      "Saving model...\n",
      "Epoch 14: train_loss: 0.2323 train_acc: 0.8618 | val_loss: 0.3173 val_acc: 0.8461\n",
      "done training tensor(0.8651, device='cuda:1')\n",
      "Saving model...\n",
      "Epoch 15: train_loss: 0.2265 train_acc: 0.8651 | val_loss: 0.3241 val_acc: 0.8523\n",
      "done training tensor(0.8712, device='cuda:1')\n",
      "Saving model...\n",
      "Epoch 16: train_loss: 0.2165 train_acc: 0.8712 | val_loss: 0.3233 val_acc: 0.8470\n",
      "done training tensor(0.8756, device='cuda:1')\n",
      "Saving model...\n",
      "Epoch 17: train_loss: 0.2092 train_acc: 0.8756 | val_loss: 0.3282 val_acc: 0.8459\n",
      "done training tensor(0.8800, device='cuda:1')\n",
      "Saving model...\n",
      "Epoch 18: train_loss: 0.2018 train_acc: 0.8800 | val_loss: 0.3516 val_acc: 0.8380\n",
      "done training tensor(0.8826, device='cuda:1')\n",
      "Saving model...\n",
      "Epoch 19: train_loss: 0.1975 train_acc: 0.8826 | val_loss: 0.3456 val_acc: 0.8357\n",
      "done training tensor(0.8853, device='cuda:1')\n",
      "Saving model...\n",
      "Epoch 20: train_loss: 0.1932 train_acc: 0.8853 | val_loss: 0.3566 val_acc: 0.8471\n"
     ]
    }
   ],
   "source": [
    "model_comp = CNN_LSTM2(vocab_size, word2vec_matrix, hidden_dim, n_layers, output_size)\n",
    "model_comp.to(device)\n",
    "train_val(model_comp, epochs, company_train_loader, img_train_loader,criterion, company_val_loader, img_val_loader, 2,4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.8485 loss: 0.3577 roc auc: 0.6495766034899384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.8485, device='cuda:1'), 0.6495766034899384)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc_auc(model_comp, company_test_loader, img_test_loader, criterion,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model that takes overview, title, production companies & poster as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train dataset\n",
    "text_train_data = TensorDataset(torch.from_numpy(X_overview_train), torch.from_numpy(X_title_train), torch.from_numpy(X_company_train),torch.from_numpy(train_labels))\n",
    "img_train_data = TensorDataset(X_img_train, torch.from_numpy(train_labels))\n",
    "\n",
    "# validation dataset\n",
    "text_val_data = TensorDataset(torch.from_numpy(X_overview_val), torch.from_numpy(X_title_val),torch.from_numpy(X_company_val), torch.from_numpy(val_labels))\n",
    "img_val_data = TensorDataset(X_img_val, torch.from_numpy(val_labels))\n",
    "\n",
    "# test dataset\n",
    "text_test_data = TensorDataset(torch.from_numpy(X_overview_test),torch.from_numpy(X_title_test), torch.from_numpy(X_company_test),torch.from_numpy(test_labels))\n",
    "img_test_data = TensorDataset(X_img_test, torch.from_numpy(test_labels))\n",
    "\n",
    "\n",
    "text_train_loader = DataLoader(text_train_data, batch_size=batch_size)\n",
    "img_train_loader = DataLoader(img_train_data, batch_size=batch_size)\n",
    "\n",
    "text_val_loader = DataLoader(text_val_data, batch_size=batch_size)\n",
    "img_val_loader = DataLoader(img_val_data, batch_size=batch_size)\n",
    "\n",
    "text_test_loader = DataLoader(text_test_data, batch_size=batch_size)\n",
    "img_test_loader = DataLoader(img_test_data, batch_size=batch_size)\n",
    " #using TensorDataset to transform the arrays into tensors for my overview textual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done training tensor(0.8142, device='cuda:1')\n",
      "Saving model...\n",
      "Epoch 1: train_loss: 0.3170 train_acc: 0.8142 | val_loss: 0.3002 val_acc: 0.8300\n",
      "done training tensor(0.8236, device='cuda:1')\n",
      "Saving model...\n",
      "Epoch 2: train_loss: 0.3018 train_acc: 0.8236 | val_loss: 0.2958 val_acc: 0.8334\n",
      "done training tensor(0.8264, device='cuda:1')\n",
      "Saving model...\n",
      "Epoch 3: train_loss: 0.2965 train_acc: 0.8264 | val_loss: 0.2955 val_acc: 0.8389\n",
      "done training tensor(0.8299, device='cuda:1')\n",
      "Saving model...\n",
      "Epoch 4: train_loss: 0.2902 train_acc: 0.8299 | val_loss: 0.2875 val_acc: 0.8377\n",
      "done training tensor(0.8338, device='cuda:1')\n",
      "Saving model...\n",
      "Epoch 5: train_loss: 0.2830 train_acc: 0.8338 | val_loss: 0.2865 val_acc: 0.8377\n",
      "done training tensor(0.8370, device='cuda:1')\n",
      "Saving model...\n",
      "Epoch 6: train_loss: 0.2773 train_acc: 0.8370 | val_loss: 0.2847 val_acc: 0.8382\n",
      "done training tensor(0.8401, device='cuda:1')\n",
      "Saving model...\n",
      "Epoch 7: train_loss: 0.2716 train_acc: 0.8401 | val_loss: 0.2862 val_acc: 0.8403\n",
      "done training tensor(0.8430, device='cuda:1')\n",
      "Saving model...\n",
      "Epoch 8: train_loss: 0.2660 train_acc: 0.8430 | val_loss: 0.2926 val_acc: 0.8447\n",
      "done training tensor(0.8453, device='cuda:1')\n",
      "Saving model...\n",
      "Epoch 9: train_loss: 0.2620 train_acc: 0.8453 | val_loss: 0.2966 val_acc: 0.8387\n",
      "done training tensor(0.8478, device='cuda:1')\n",
      "Saving model...\n",
      "Epoch 10: train_loss: 0.2573 train_acc: 0.8478 | val_loss: 0.3036 val_acc: 0.8381\n",
      "done training tensor(0.8514, device='cuda:1')\n",
      "Saving model...\n",
      "Epoch 11: train_loss: 0.2507 train_acc: 0.8514 | val_loss: 0.3062 val_acc: 0.8396\n",
      "done training tensor(0.8544, device='cuda:1')\n",
      "Saving model...\n",
      "Epoch 12: train_loss: 0.2453 train_acc: 0.8544 | val_loss: 0.3141 val_acc: 0.8412\n",
      "done training tensor(0.8586, device='cuda:1')\n",
      "Saving model...\n",
      "Epoch 13: train_loss: 0.2381 train_acc: 0.8586 | val_loss: 0.3215 val_acc: 0.8389\n",
      "done training tensor(0.8615, device='cuda:1')\n",
      "Saving model...\n",
      "Epoch 14: train_loss: 0.2331 train_acc: 0.8615 | val_loss: 0.3145 val_acc: 0.8405\n",
      "done training tensor(0.8647, device='cuda:1')\n",
      "Saving model...\n",
      "Epoch 15: train_loss: 0.2274 train_acc: 0.8647 | val_loss: 0.3307 val_acc: 0.8464\n",
      "done training tensor(0.8708, device='cuda:1')\n",
      "Saving model...\n",
      "Epoch 16: train_loss: 0.2170 train_acc: 0.8708 | val_loss: 0.3373 val_acc: 0.8503\n",
      "done training tensor(0.8752, device='cuda:1')\n",
      "Saving model...\n",
      "Epoch 17: train_loss: 0.2098 train_acc: 0.8752 | val_loss: 0.3374 val_acc: 0.8457\n",
      "done training tensor(0.8779, device='cuda:1')\n",
      "Saving model...\n",
      "Epoch 18: train_loss: 0.2057 train_acc: 0.8779 | val_loss: 0.3246 val_acc: 0.8362\n",
      "done training tensor(0.8807, device='cuda:1')\n",
      "Saving model...\n",
      "Epoch 19: train_loss: 0.2010 train_acc: 0.8807 | val_loss: 0.3340 val_acc: 0.8439\n",
      "done training tensor(0.8848, device='cuda:1')\n",
      "Saving model...\n",
      "Epoch 20: train_loss: 0.1934 train_acc: 0.8848 | val_loss: 0.3554 val_acc: 0.8491\n"
     ]
    }
   ],
   "source": [
    "model4 = CNN_LSTM4sc(vocab_size, word2vec_matrix, hidden_dim, n_layers, output_size)\n",
    "model4.to(device)\n",
    "train_val(model4, epochs, text_train_loader, img_train_loader, criterion, text_val_loader, img_val_loader, 4,5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4sum = CNN_LSTM4sc(vocab_size, word2vec_matrix, hidden_dim, n_layers, output_size,concat=False) # try summing\n",
    "model4sum.to(device)\n",
    "train_val(model4sum, epochs, text_train_loader, img_train_loader, criterion, text_val_loader, img_val_loader, 4,7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f3f8a50d750>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#action, comedy, crime, drama the lowest, horror, romance, thriller\n",
    "\n",
    "l = df.genres.values\n",
    "\n",
    "c = [list(combinations(i,2)) for i in l]\n",
    "a = list(itertools.chain.from_iterable((i, i[::-1]) for c_ in c for i in c_))\n",
    "dft = pd.DataFrame(a)\n",
    "aba=pd.pivot_table(dft, index=0, columns=1, aggfunc='size', fill_value=0)\n",
    "sns.heatmap(aba,cmap=\"YlGn\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
