{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing modules I will use\n",
    "import pandas as pd\n",
    "import json\n",
    "from collections import defaultdict, Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "from itertools import combinations\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import numpy as np\n",
    "import ast\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from gensim.models import KeyedVectors\n",
    "import string\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "device = torch.device('cuda:0') #if torch.cuda.is_available() else 'cpu')\n",
    "from PIL import Image\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/df_subm.csv')\n",
    "# The split to val, test and train has taken place in the ai_project_preprocessing.ipynb\n",
    "val_df = pd.read_csv('../data/val_subm.csv')\n",
    "test_df = pd.read_csv('../data/test_subm.csv')\n",
    "train_df = pd.read_csv('../data/train_subm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have the tensors saved locally, I can upload them on my repo too\n",
    "# I usually start things from this point\n",
    "\n",
    "val_tensors = torch.load('../data/val_subm.pt')\n",
    "test_tensors = torch.load('../data/test_subm.pt')\n",
    "train_tensors = torch.load('../data/train_subm.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['genres']=df['genres'].apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "train_df['genres']=train_df['genres'].apply(lambda x: ast.literal_eval(x))\n",
    "test_df['genres']=test_df['genres'].apply(lambda x: ast.literal_eval(x))\n",
    "val_df['genres']=val_df['genres'].apply(lambda x: ast.literal_eval(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['production_companies']=train_df['production_companies'].apply(lambda x: ast.literal_eval(x))\n",
    "test_df['production_companies']=test_df['production_companies'].apply(lambda x: ast.literal_eval(x))\n",
    "val_df['production_companies']=val_df['production_companies'].apply(lambda x: ast.literal_eval(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['production_companies']=train_df['production_companies'].apply(lambda x: (' ').join(x))\n",
    "test_df['production_companies']=test_df['production_companies'].apply(lambda x: (' ').join(x))\n",
    "val_df['production_companies']=val_df['production_companies'].apply(lambda x: (' ').join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "# We need to pass a list of lists of the labels to get a one-hot encoding for the train, the test and the validation dataset.\n",
    "\n",
    "# Fitting the multilabel binarizer to the labels available.\n",
    "train_labels = mlb.fit_transform(train_df['genres'].tolist())\n",
    "test_labels = mlb.fit_transform(test_df['genres'].tolist())\n",
    "val_labels = mlb.fit_transform(val_df['genres'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function creates a dictionary that has the number of movie labels as a key \n",
    "# and the number of occurrences of this number of labels as a value.\n",
    "\n",
    "def plot_num_labels_movies(genre_lists):\n",
    "    dic = {}\n",
    "    for genre_list in genre_lists:\n",
    "        if len(genre_list) in dic:\n",
    "            dic[len(genre_list)] += 1\n",
    "            \n",
    "        else:\n",
    "            dic[len(genre_list)] = 1\n",
    "    return dic \n",
    "\n",
    "plot_dic = plot_num_labels_movies(df.genres.tolist())\n",
    "plt.bar(*zip(*plot_dic.items()))\n",
    "plt.title('Occurrences of different number of labels in the dataset')\n",
    "plt.xlabel('Number of labels')\n",
    "plt.ylabel('Number of occurrences')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swords = stopwords.words('english')\n",
    "reg_ex_tokenizer = RegexpTokenizer(r'\\w+')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean(text,is_for_summary=True):\n",
    "    # removing punctuation and digits\n",
    "    text = ''.join([c for c in text if c not in string.punctuation and not c.isdigit()])\n",
    "    # tokenizing with the RegexpTokenizer and lowercasing the text\n",
    "    text = reg_ex_tokenizer.tokenize(text.lower())\n",
    "    # removing stop words if cleaning is applied on the overview - if we clean up the titles we keep the stop words since certain titles contain solely stop words(e.g. 'Who am I?').\n",
    "    # maybe I should filter out stop words in titles too and only keep them when there are no other words.\n",
    "    if is_for_summary:        \n",
    "        text = [w for w in text if w not in swords]\n",
    "    # lemmatizing the words with the WordNetLemmatizer and using join to put them back into a string\n",
    "    text = ' '.join([lemmatizer.lemmatize(w) for w in text])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning up the overview column.\n",
    "train_df['overview'] = train_df['overview'].apply(lambda x: clean(x))\n",
    "test_df['overview'] = test_df['overview'].apply(lambda x: clean(x))\n",
    "val_df['overview'] = val_df['overview'].apply(lambda x: clean(x))\n",
    "\n",
    "# Cleaning up the original title column.\n",
    "train_df['original_title'] = train_df['original_title'].apply(lambda x: clean(x,False))\n",
    "test_df['original_title'] = test_df['original_title'].apply(lambda x: clean(x,False))\n",
    "val_df['original_title'] = val_df['original_title'].apply(lambda x: clean(x,False))\n",
    "\n",
    "# Cleaning up the production companies column.\n",
    "train_df['production_companies'] = train_df['production_companies'].apply(lambda x: clean(x))\n",
    "test_df['production_companies'] = test_df['production_companies'].apply(lambda x: clean(x))\n",
    "val_df['production_companies'] = val_df['production_companies'].apply(lambda x: clean(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the three columns containing textual data into one to cover all possible occurrences for words.\n",
    "df_tit_overview = df[['original_title','overview','production_companies']] \n",
    "df_vocab=df_tit_overview.stack().reset_index()\n",
    "# Getting length of sequences for padding\n",
    "max_len_for_padding = df_vocab[0].map(len).max()\n",
    "embedding_dim = 300\n",
    "tokenizer = Tokenizer(num_words=50000, lower=True)\n",
    "tokenizer.fit_on_texts(df_vocab[0].values)\n",
    "# Creating a word to integer dictionary for my vocab.\n",
    "word_index = tokenizer.word_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_vocab(pretrained_dict, vocab, emb_size):\n",
    "    # creating a dictionary with keys the words and values the pretrained word vectors. \n",
    "    embeddings_index = {}\n",
    "    for w in pretrained_dict.wv.vocab:\n",
    "        embeddings_index[w] = pretrained_dict.word_vec(w)     \n",
    "    # instantiating matrix with shape (vocab + 1, 300)\n",
    "    embedding_matrix = 1 * np.random.randn(len(vocab)+1, emb_size)\n",
    "    # looking up the words in my vocab\n",
    "    for word, i in vocab.items():\n",
    "        i-=1\n",
    "        # getting the pretrained vector for the corresponding word\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # adding the vector to the matrix\n",
    "            embedding_matrix[i] = embedding_vector                      \n",
    "    del(embeddings_index)\n",
    "        \n",
    "    return embedding_matrix\n",
    "\n",
    "# need to download the pretrained vectors file. uncomment to download and unzip\n",
    "#!wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n",
    "#!gunzip /content/GoogleNews-vectors-negative300.bin.gz\n",
    "\n",
    "w2v_dic = KeyedVectors.load_word2vec_format(\"/home/gusmavko@GU.GU.SE/aics-project/data/GoogleNews-vectors-negative300.bin\", binary=True)\n",
    "word2vec_matrix = embed_vocab(w2v_dic, word_index, 300)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming each word token from the textual data into the corresponding index. \n",
    "# TODO maybe when padding do I need to pad the titles to the same length as the overview \n",
    "\n",
    "X_overview_train = tokenizer.texts_to_sequences(train_df['overview'].values)\n",
    "X_overview_train = pad_sequences(X_overview_train, maxlen=max_len_for_padding)\n",
    "\n",
    "X_overview_test = tokenizer.texts_to_sequences(test_df['overview'].values)\n",
    "X_overview_test = pad_sequences(X_overview_test, maxlen=max_len_for_padding)\n",
    "\n",
    "X_overview_val = tokenizer.texts_to_sequences(val_df['overview'].values)\n",
    "X_overview_val = pad_sequences(X_overview_val, maxlen=max_len_for_padding)\n",
    "\n",
    "print(f'Train overview shape:{X_overview_train.shape}, Test overview shape:{X_overview_test.shape}, Validation overview shape:{X_overview_val.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tranforming the titles to integer representations.\n",
    "max_len_title = train_df['original_title'].map(len).max()\n",
    "# train title\n",
    "\n",
    "X_title_train = tokenizer.texts_to_sequences(train_df['original_title'].values)\n",
    "X_title_train = pad_sequences(X_title_train,maxlen=max_len_title)\n",
    "# test title\n",
    "\n",
    "X_title_test = tokenizer.texts_to_sequences(test_df['original_title'].values)\n",
    "X_title_test = pad_sequences(X_title_test,maxlen=max_len_title)\n",
    "\n",
    "# val title\n",
    "X_title_val = tokenizer.texts_to_sequences(val_df['original_title'].values)\n",
    "X_title_val = pad_sequences(X_title_val,maxlen=max_len_title)\n",
    "\n",
    "print(f'Train title shape:{X_title_train.shape}, Test overview shape:{X_title_test.shape}, Validation overview shape:{X_title_val.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming the companies to integer representations\n",
    "max_len_comp = train_df['production_companies'].map(len).max()\n",
    "X_company_train = tokenizer.texts_to_sequences(train_df['production_companies'].values)\n",
    "X_company_train = pad_sequences(X_company_train, maxlen=max_len_comp)\n",
    "\n",
    "X_company_test = tokenizer.texts_to_sequences(test_df['production_companies'].values)\n",
    "X_company_test = pad_sequences(X_company_test, maxlen=max_len_comp)\n",
    "\n",
    "X_company_val = tokenizer.texts_to_sequences(val_df['production_companies'].values)\n",
    "X_company_val = pad_sequences(X_company_val, maxlen=max_len_comp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the dimensions of the tensors to later pass them in batches to the model.\n",
    "X_img_val = val_tensors.permute(0,3,1,2)\n",
    "\n",
    "X_img_test = test_tensors.permute(0,3,1,2)\n",
    "\n",
    "X_img_train = train_tensors.permute(0,3,1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 150\n",
    "img_train_data = TensorDataset(X_img_train, torch.from_numpy(train_labels))\n",
    "img_val_data = TensorDataset(X_img_val, torch.from_numpy(val_labels))\n",
    "img_test_data = TensorDataset(X_img_test, torch.from_numpy(test_labels))\n",
    "\n",
    "# Dataloaders for the images\n",
    "img_train_loader = DataLoader(img_train_data, batch_size=batch_size)\n",
    "img_val_loader = DataLoader(img_val_data, batch_size=batch_size)\n",
    "img_test_loader = DataLoader(img_test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using TensorDataset to transform the arrays into tensors for my overview title data\n",
    "\n",
    "title_train_data = TensorDataset(torch.from_numpy(X_title_train), torch.from_numpy(train_labels))\n",
    "title_val_data = TensorDataset(torch.from_numpy(X_title_val), torch.from_numpy(val_labels))\n",
    "title_test_data = TensorDataset(torch.from_numpy(X_title_test), torch.from_numpy(test_labels))\n",
    "\n",
    "title_train_loader = DataLoader(title_train_data, batch_size=batch_size)\n",
    "title_val_loader = DataLoader(title_val_data, batch_size=batch_size)\n",
    "title_test_loader = DataLoader(title_test_data, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using TensorDataset to transform the arrays into tensors for my overview textual data\n",
    "overview_train_data = TensorDataset(torch.from_numpy(X_overview_train), torch.from_numpy(train_labels))\n",
    "overview_val_data = TensorDataset(torch.from_numpy(X_overview_val), torch.from_numpy(val_labels))\n",
    "overview_test_data = TensorDataset(torch.from_numpy(X_overview_test), torch.from_numpy(test_labels))\n",
    "\n",
    "overview_train_loader = DataLoader(overview_train_data, batch_size=batch_size)\n",
    "overview_val_loader = DataLoader(overview_val_data, batch_size=batch_size)\n",
    "overview_test_loader = DataLoader(overview_test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using TensorDataset to transform the arrays into tensors for my overview textual data\n",
    "company_train_data = TensorDataset(torch.from_numpy(X_company_train), torch.from_numpy(train_labels))\n",
    "company_val_data = TensorDataset(torch.from_numpy(X_company_val), torch.from_numpy(val_labels))\n",
    "company_test_data = TensorDataset(torch.from_numpy(X_company_test), torch.from_numpy(test_labels))\n",
    "\n",
    "company_train_loader = DataLoader(company_train_data, batch_size=batch_size)\n",
    "company_val_loader = DataLoader(company_val_data, batch_size=batch_size)\n",
    "company_test_loader = DataLoader(company_test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, using TensorDataset to transform the arrays into tensors for my textual data.\n",
    "\n",
    "# train dataset\n",
    "three_feats_train_data = TensorDataset(torch.from_numpy(X_overview_train), torch.from_numpy(X_title_train), torch.from_numpy(train_labels))\n",
    "# validation dataset\n",
    "three_feats_val_data = TensorDataset(torch.from_numpy(X_overview_val), torch.from_numpy(X_title_val), torch.from_numpy(val_labels))\n",
    "# test dataset\n",
    "three_feats_test_data = TensorDataset(torch.from_numpy(X_overview_test),torch.from_numpy(X_title_test), torch.from_numpy(test_labels))\n",
    "\n",
    "# dataloader for the model that uses three features as input\n",
    "three_feats_train_loader = DataLoader(three_feats_train_data, batch_size=batch_size)\n",
    "three_feats_val_loader = DataLoader(three_feats_val_data, batch_size=batch_size)\n",
    "three_feats_test_loader = DataLoader(three_feats_test_data, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train dataset\n",
    "four_feats_train_data = TensorDataset(torch.from_numpy(X_overview_train), torch.from_numpy(X_title_train), torch.from_numpy(X_company_train),torch.from_numpy(train_labels))\n",
    "# validation dataset\n",
    "four_feats_val_data = TensorDataset(torch.from_numpy(X_overview_val), torch.from_numpy(X_title_val),torch.from_numpy(X_company_val), torch.from_numpy(val_labels))\n",
    "# test dataset\n",
    "four_feats_test_data = TensorDataset(torch.from_numpy(X_overview_test),torch.from_numpy(X_title_test), torch.from_numpy(X_company_test),torch.from_numpy(test_labels))\n",
    "\n",
    "# Dataloader\n",
    "four_feats_train_loader = DataLoader(four_feats_train_data, batch_size=batch_size)\n",
    "four_feats_val_loader = DataLoader(four_feats_val_data, batch_size=batch_size)\n",
    "four_feats_test_loader = DataLoader(four_feats_test_data, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 150\n",
    "epochs = 20\n",
    "clip = 5\n",
    "vocab_size = len(word_index)+1\n",
    "output_size = train_labels.shape[1]\n",
    "embedding_dim = 300\n",
    "hidden_dim = 64\n",
    "n_layers = 2\n",
    "lr=0.001\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model that takes 2 features as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building model for movie genre classification using textual and image features.\n",
    "# CNN_LSTM2sc takes two features as input, The multimodal features can be either concatenated or summed. Change self.concat to False to sum.\n",
    "class CNN_LSTM2sc(nn.Module):\n",
    "    def __init__(self, vocab_size, weights_matrix, n_hidden, n_layers, n_out, concat=True):\n",
    "        super(CNN_LSTM2sc, self).__init__()\n",
    "\n",
    "        # LSTM for the text\n",
    "        self.vocab_size, self.n_hidden, self.n_out, self.n_layers = vocab_size, n_hidden, n_out, n_layers\n",
    "        num_embeddings, embedding_dim = weights_matrix.shape[0], weights_matrix.shape[1]\n",
    "        self.emb = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.emb.weight.data.copy_(torch.from_numpy(weights_matrix))\n",
    "        self.emb.weight.requires_grad = True\n",
    "        self.lstm = nn.LSTM(embedding_dim, self.n_hidden, self.n_layers, dropout=0.2, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.lstm_fc = nn.Linear(self.n_hidden, 128)\n",
    "        self.concat = concat\n",
    "        \n",
    "        # CNN for the posters\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3) # Gives me depth of input.\n",
    "        # pooling layers to reduce the image size\n",
    "        self.max_pool1 = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3)\n",
    "        self.max_pool2 = nn.MaxPool2d(2)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3)\n",
    "        self.max_pool3 = nn.MaxPool2d(2)\n",
    "        self.conv4 = nn.Conv2d(128, 128, 3)\n",
    "        self.max_pool4 = nn.MaxPool2d(2)\n",
    "        # to drop the nodes that are below the wanted weights\n",
    "        self.cnn_dropout = nn.Dropout(0.1)\n",
    "        if self.concat == True:\n",
    "            self.cnn_fc = nn.Linear(4*4*128, 512) # Gives me number of features of input.         \n",
    "            self.combined_fc1 = nn.Linear(640, 256) # Give me features of input.\n",
    "            self.combined_fc2 = nn.Linear(256, 128)\n",
    "            self.output_fc = nn.Linear(128, n_out)\n",
    "        \n",
    "        else:            \n",
    "            self.cnn_fc = nn.Linear(4*4*128, 128) # when we sum, we want our textual and visual features to be of the same size so we can stack them\n",
    "            # Concat layer for the combined feature space\n",
    "            self.combined_fc1 = nn.Linear(128, 64) # Give me features of input.\n",
    "            self.combined_fc2 = nn.Linear(64, 32)\n",
    "            self.output_fc = nn.Linear(32, n_out)\n",
    "\n",
    "\n",
    "    def forward(self, lstm_inp, cnn_inp):\n",
    "        batch_size = lstm_inp.size(0)\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        lstm_inp = lstm_inp.long()\n",
    "        embeds = self.emb(lstm_inp)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        lstm_out = self.dropout(lstm_out[:, -1])\n",
    "        lstm_out = F.relu(self.lstm_fc(lstm_out))\n",
    "        \n",
    "        x = F.relu(self.conv1(cnn_inp))\n",
    "        x = self.max_pool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.max_pool2(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.max_pool3(x)\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.max_pool4(x)\n",
    "        # that is how I get the correct dimensions for the CNN output that is then passed as input to the linear layer\n",
    "        # need the input to be two dimensional again\n",
    "        x = x.view(-1, 4*4*128)\n",
    "        x = self.cnn_dropout(x)\n",
    "        \n",
    "        if self.concat == True:\n",
    "            cnn_out = F.relu(self.cnn_fc(x))            \n",
    "            #concatenating the cnn output and the lstm output\n",
    "            combined_inp = torch.cat((cnn_out, lstm_out), 1)\n",
    "            # activation function\n",
    "            x_comb = F.relu(self.combined_fc1(combined_inp))\n",
    "            x_comb = F.relu(self.combined_fc2(x_comb))\n",
    "            # sigmoid to get probabilities for each label\n",
    "            out = torch.sigmoid(self.output_fc(x_comb))\n",
    "        else: # sum happens here\n",
    "            cnn_out = F.relu(self.cnn_fc(x))            \n",
    "            combined_inp = torch.stack([cnn_out, lstm_out], 1)            \n",
    "            sum_comb = torch.sum(combined_inp,dim=1)           \n",
    "            # activation function\n",
    "            x_comb = F.relu(self.combined_fc1(sum_comb))\n",
    "            x_comb = F.relu(self.combined_fc2(x_comb))\n",
    "            # sigmoid to get probabilities for each label\n",
    "            out = torch.sigmoid(self.output_fc(x_comb))     \n",
    "                              \n",
    "        return out\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device),\n",
    "                          weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device))\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model that takes 3 features as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building model for movie genre classification using textual and image features.\n",
    "# CNN_LSTM3sc takes three features as input. The multimodal features can be either concatenated or summed. Change self.concat to False to sum.\n",
    "class CNN_LSTM3sc(nn.Module):\n",
    "    def __init__(self, vocab_size, weights_matrix, n_hidden, n_layers, n_out, concat=True):\n",
    "        super(CNN_LSTM3sc, self).__init__()\n",
    "\n",
    "        # LSTM for the text\n",
    "        self.vocab_size, self.n_hidden, self.n_out, self.n_layers = vocab_size, n_hidden, n_out, n_layers\n",
    "        num_embeddings, embedding_dim = weights_matrix.shape[0], weights_matrix.shape[1]\n",
    "        self.emb = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.emb.weight.data.copy_(torch.from_numpy(weights_matrix))\n",
    "        self.emb.weight.requires_grad = True\n",
    "        self.lstm = nn.LSTM(embedding_dim, self.n_hidden, self.n_layers, dropout=0.2, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.lstm_fc = nn.Linear(self.n_hidden, 128)\n",
    "        self.concat = concat\n",
    "        \n",
    "        # CNN for the posters\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3) # Gives me depth of input.\n",
    "        # pooling layers to reduce the image size\n",
    "        self.max_pool1 = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3)\n",
    "        self.max_pool2 = nn.MaxPool2d(2)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3)\n",
    "        self.max_pool3 = nn.MaxPool2d(2)\n",
    "        self.conv4 = nn.Conv2d(128, 128, 3)\n",
    "        self.max_pool4 = nn.MaxPool2d(2)\n",
    "        # to drop the nodes that are below the wanted weights\n",
    "        self.cnn_dropout = nn.Dropout(0.1)\n",
    "        if self.concat == True:\n",
    "            self.cnn_fc = nn.Linear(4*4*128, 512) # Gives me number of features of input.\n",
    "        # Concat layer for the combined feature space\n",
    "            self.combined_fc1 = nn.Linear(640, 256) # Give me features of input.\n",
    "            self.combined_fc2 = nn.Linear(256, 128)\n",
    "            self.output_fc = nn.Linear(128, n_out)        \n",
    "        else:            \n",
    "            self.cnn_fc = nn.Linear(4*4*128, 128) # when we sum, we want our textual and visual features to be of the same size so we can stack them\n",
    "            # Concat layer for the combined feature space\n",
    "            self.combined_fc1 = nn.Linear(128, 64) # Give me features of input.\n",
    "            self.combined_fc2 = nn.Linear(64, 32)\n",
    "            self.output_fc = nn.Linear(32, n_out)\n",
    "\n",
    "    def forward(self, lstm_in, cnn_inp, title_inp):\n",
    "        batch_size = lstm_in.size(0)\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        lstm_in = lstm_in.long()\n",
    "        title_inp = title_inp.long()\n",
    "        embeds_lstm = self.emb(lstm_in)\n",
    "        embeds_title = self.emb(title_inp)\n",
    "        embeds = torch.cat((embeds_lstm,embeds_title),dim=1)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        lstm_out = self.dropout(lstm_out[:, -1])\n",
    "        lstm_out = F.relu(self.lstm_fc(lstm_out))\n",
    "       \n",
    "        x = F.relu(self.conv1(cnn_inp))\n",
    "        x = self.max_pool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.max_pool2(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.max_pool3(x)\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.max_pool4(x)\n",
    "        # need the input to be two dimensional again\n",
    "        x = x.view(-1, 4*4*128)        \n",
    "        x = self.cnn_dropout(x)        \n",
    "        if self.concat == True: # concatenate the multimodal features\n",
    "            cnn_out = F.relu(self.cnn_fc(x))            \n",
    "            #concatenating the cnn output and the lstm output\n",
    "            combined_inp = torch.cat((cnn_out, lstm_out), 1)\n",
    "            # activation function\n",
    "            x_comb = F.relu(self.combined_fc1(combined_inp))\n",
    "            x_comb = F.relu(self.combined_fc2(x_comb))\n",
    "            # sigmoid to get probabilities for each label\n",
    "            out = torch.sigmoid(self.output_fc(x_comb))\n",
    "        else: # sum the multimodal features\n",
    "            cnn_out = F.relu(self.cnn_fc(x))\n",
    "            # sum happens here\n",
    "            combined_inp = torch.stack([cnn_out, lstm_out], 1)            \n",
    "            sum_comb = torch.sum(combined_inp,dim=1)\n",
    "            # activation function\n",
    "            x_comb = F.relu(self.combined_fc1(sum_comb))\n",
    "            x_comb = F.relu(self.combined_fc2(x_comb))\n",
    "            # # sigmoid to get probabilities for each label\n",
    "            out = torch.sigmoid(self.output_fc(x_comb))     \n",
    "                              \n",
    "        return out\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device),\n",
    "                          weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device))\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model that takes 4 features as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building model for movie genre classification using textual and image features.\n",
    "# CNN_LSTM4sc takes four features as input. The multimodal features can be either concatenated or summed. Change self.concat to False to sum.\n",
    "class CNN_LSTM4sc(nn.Module):\n",
    "    def __init__(self, vocab_size, weights_matrix, n_hidden, n_layers, n_out, concat=True):\n",
    "        super(CNN_LSTM4sc, self).__init__()\n",
    "\n",
    "        # LSTM for the text\n",
    "        self.vocab_size, self.n_hidden, self.n_out, self.n_layers = vocab_size, n_hidden, n_out, n_layers\n",
    "        num_embeddings, embedding_dim = weights_matrix.shape[0], weights_matrix.shape[1]\n",
    "        self.emb = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.emb.weight.data.copy_(torch.from_numpy(weights_matrix))\n",
    "        self.emb.weight.requires_grad = True\n",
    "        self.lstm = nn.LSTM(embedding_dim, self.n_hidden, self.n_layers, dropout=0.2, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.lstm_fc = nn.Linear(self.n_hidden, 128)\n",
    "        self.concat = concat\n",
    "        \n",
    "        # CNN for the posters\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3) # Gives me depth of input.\n",
    "        # pooling layers to reduce the image size\n",
    "        self.max_pool1 = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3)\n",
    "        self.max_pool2 = nn.MaxPool2d(2)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3)\n",
    "        self.max_pool3 = nn.MaxPool2d(2)\n",
    "        self.conv4 = nn.Conv2d(128, 128, 3)\n",
    "        self.max_pool4 = nn.MaxPool2d(2)\n",
    "        # to drop the nodes that are below the wanted weights\n",
    "        self.cnn_dropout = nn.Dropout(0.1)\n",
    "        if self.concat == True: # concatenate the multimodal features\n",
    "            self.cnn_fc = nn.Linear(4*4*128, 512) # Gives me number of features of input.  \n",
    "            # Concat layer for the combined feature space\n",
    "            self.combined_fc1 = nn.Linear(640, 256) # Give me features of input.\n",
    "            self.combined_fc2 = nn.Linear(256, 128)\n",
    "            self.output_fc = nn.Linear(128, n_out)        \n",
    "        else:   # sum the multimodal features\n",
    "            self.cnn_fc = nn.Linear(4*4*128, 128) # when we sum, we want our textual and visual features to be of the same size so we can stack them\n",
    "            # Concat layer for the combined feature space\n",
    "            self.combined_fc1 = nn.Linear(128, 64) # Give me features of input.\n",
    "            self.combined_fc2 = nn.Linear(64, 32)\n",
    "            self.output_fc = nn.Linear(32, n_out)\n",
    "\n",
    "\n",
    "    def forward(self, lstm_in, cnn_inp, title_inp, company_inp):\n",
    "        batch_size = lstm_in.size(0)\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        lstm_in, title_inp,company_inp = lstm_in.long(), title_inp.long(), company_inp.long()\n",
    "        embeds_lstm = self.emb(lstm_in)\n",
    "        embeds_title = self.emb(title_inp)\n",
    "        embeds_company = self.emb(company_inp)\n",
    "        embeds = torch.cat((embeds_lstm,embeds_title,embeds_company),dim=1)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        lstm_out = self.dropout(lstm_out[:, -1])\n",
    "        lstm_out = F.relu(self.lstm_fc(lstm_out))\n",
    "\n",
    "        x = F.relu(self.conv1(cnn_inp))\n",
    "        x = self.max_pool1(x)\n",
    "        \n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.max_pool2(x)\n",
    "        \n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.max_pool3(x)\n",
    "        \n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.max_pool4(x)\n",
    "        # that is how I get the correct dimensions for the CNN output that is then passed as input to the linear layer\n",
    "        # need the input to be two dimensional again\n",
    "        x = x.view(-1, 4*4*128)\n",
    "        \n",
    "        x = self.cnn_dropout(x)\n",
    "        if self.concat == True: # concatenate the multimodal features\n",
    "            cnn_out = F.relu(self.cnn_fc(x))            \n",
    "            #concatenating the cnn output and the lstm output\n",
    "            combined_inp = torch.cat((cnn_out, lstm_out), 1)\n",
    "            # activation function\n",
    "            x_comb = F.relu(self.combined_fc1(combined_inp))\n",
    "            x_comb = F.relu(self.combined_fc2(x_comb))\n",
    "            # sigmoid to get probabilities for each label\n",
    "            out = torch.sigmoid(self.output_fc(x_comb))\n",
    "        else: # sum the multimodal features\n",
    "            cnn_out = F.relu(self.cnn_fc(x))\n",
    "            combined_inp = torch.stack([cnn_out, lstm_out], 1)            \n",
    "            sum_comb = torch.sum(combined_inp,dim=1)\n",
    "            # activation function\n",
    "            x_comb = F.relu(self.combined_fc1(sum_comb))\n",
    "            x_comb = F.relu(self.combined_fc2(x_comb))\n",
    "            # sigmoid to get 1 and 0 on the one-hot-encoding formatted label\n",
    "            out = torch.sigmoid(self.output_fc(x_comb))     \n",
    "                              \n",
    "        return out\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device),\n",
    "                          weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device))\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_feats can be either 2,3 or 4 – i.e. the number of different features (overview, title, companies, poster) used as input to the model.\n",
    "# model_num is an integer (0-9) to save the models we train.\n",
    "def train_val(model,epochs, t_train_loader, image_train_loader, criterion, t_val_loader, image_val_loader, num_feats, model_num):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    model.train()\n",
    "    for i in range(epochs):\n",
    "        total_acc_train = 0\n",
    "        total_loss_train = 0        \n",
    "        for lstm, cnn in zip(t_train_loader, image_train_loader):\n",
    "            cnn_inp, cnn_labels = cnn\n",
    "            cnn_inp = cnn_inp.float()\n",
    "            cnn_inp, cnn_labels = cnn_inp.to(device), cnn_labels.to(device)\n",
    "            if num_feats == 2:\n",
    "                lstm_inp,lstm_labels = lstm\n",
    "                lstm_inp = lstm_inp.float()                \n",
    "                lstm_inp, lstm_labels = lstm_inp.to(device), lstm_labels.to(device)                \n",
    "                model.zero_grad()\n",
    "                output = model(lstm_inp, cnn_inp)                \n",
    "            elif num_feats == 3:\n",
    "                lstm_inp, title_inp, lstm_labels = lstm        \n",
    "                title_inp = title_inp.float()\n",
    "                lstm_inp = lstm_inp.float()               \n",
    "                lstm_inp = lstm_inp.to(device)\n",
    "                title_inp = title_inp.to(device)\n",
    "                lstm_labels =  lstm_labels.to(device)      \n",
    "                model.zero_grad()\n",
    "                output = model(lstm_inp, cnn_inp, title_inp)                \n",
    "            else:\n",
    "                lstm_inp, title_inp, company_inp, lstm_labels = lstm\n",
    "                lstm_inp = lstm_inp.float()\n",
    "                title_inp = title_inp.float()\n",
    "                company_inp = company_inp.float()                \n",
    "                lstm_inp, title_inp, company_inp,lstm_labels = lstm_inp.to(device),title_inp.to(device), company_inp.to(device), lstm_labels.to(device)\n",
    "                \n",
    "                model.zero_grad()\n",
    "                output = model(lstm_inp, cnn_inp, title_inp,company_inp)\n",
    "                \n",
    "            loss = criterion(output.squeeze(), lstm_labels.float())\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            optimizer.step()\n",
    "        \n",
    "            with torch.no_grad():\n",
    "                acc = torch.abs(output.squeeze() - lstm_labels.float()).view(-1)\n",
    "                acc = (1. - acc.sum() / acc.size()[0])\n",
    "                total_acc_train += acc\n",
    "                total_loss_train += loss.item()\n",
    "      \n",
    "        train_acc = total_acc_train/len(t_train_loader)\n",
    "        train_loss = total_loss_train/len(t_train_loader)\n",
    "        print('done training',train_acc)\n",
    "        model.eval()\n",
    "        total_acc_val = 0\n",
    "        total_loss_val = 0\n",
    "        with torch.no_grad():\n",
    "            for lstm, cnn in zip(t_val_loader, image_val_loader):\n",
    "                cnn_inp, cnn_labels = cnn\n",
    "                cnn_inp = cnn_inp.float()\n",
    "                cnn_inp, cnn_labels = cnn_inp.to(device), cnn_labels.to(device)\n",
    "                if num_feats == 2:\n",
    "                    lstm_inp,lstm_labels = lstm\n",
    "                    lstm_inp = lstm_inp.float()                \n",
    "                    lstm_inp, lstm_labels = lstm_inp.to(device), lstm_labels.to(device)                \n",
    "                    model.zero_grad()\n",
    "                    output = model(lstm_inp, cnn_inp) \n",
    "                elif num_feats == 3:\n",
    "                    lstm_inp, title_inp, lstm_labels = lstm        \n",
    "                    title_inp = title_inp.float()\n",
    "                    lstm_inp = lstm_inp.float()               \n",
    "                    lstm_inp = lstm_inp.to(device)\n",
    "                    title_inp = title_inp.to(device)\n",
    "                    lstm_labels =  lstm_labels.to(device)      \n",
    "                    model.zero_grad()\n",
    "                    output = model(lstm_inp, cnn_inp,title_inp)\n",
    "                else:\n",
    "                    lstm_inp, title_inp, company_inp, lstm_labels = lstm\n",
    "                    lstm_inp = lstm_inp.float()\n",
    "                    title_inp = title_inp.float()\n",
    "                    company_inp = company_inp.float()                \n",
    "                    lstm_inp, title_inp, company_inp,lstm_labels = lstm_inp.to(device),title_inp.to(device), company_inp.to(device), lstm_labels.to(device)\n",
    "                    model.zero_grad()\n",
    "                    output = model(lstm_inp, cnn_inp, title_inp,company_inp)\n",
    "                    \n",
    "                    \n",
    "                val_loss = criterion(output.squeeze(), lstm_labels.float())\n",
    "                acc = torch.abs(output.squeeze() - lstm_labels.float()).view(-1)\n",
    "                acc = (1. - acc.sum() / acc.size()[0])\n",
    "                total_acc_val += acc\n",
    "                total_loss_val += val_loss.item()\n",
    "        print(\"Saving model...\") \n",
    "        # 10 diff models to save\n",
    "        if model_num == 0: # concatenated title and poster\n",
    "            torch.save(model.state_dict(), '../data/models/model0.pt')\n",
    "        elif model_num == 1: # concatenated overview and poster\n",
    "            torch.save(model.state_dict(), '../data/models/model1.pt')    \n",
    "        elif model_num == 2: # concatenated overview, title and poster\n",
    "            torch.save(model.state_dict(), '../data/models/model2.pt')\n",
    "        elif model_num == 3: # concatenated companies and poster\n",
    "            torch.save(model.state_dict(), '../data/models/model3.pt')\n",
    "        elif model_num == 4: # concatenated overview, title, companies and poster\n",
    "            torch.save(model.state_dict(), '../data/models/model4.pt')\n",
    "        elif model_num == 5: # summed title and poster\n",
    "            torch.save(model.state_dict(), '../data/models/model5.pt')\n",
    "        elif model_num == 6: # summed overview and poster\n",
    "            torch.save(model.state_dict(), '../data/models/model6.pt')\n",
    "        elif model_num == 7: # summed overview, title and poster\n",
    "            torch.save(model.state_dict(), '../data/models/model7.pt')\n",
    "        elif model_num == 8: # summed companies and poster\n",
    "            torch.save(model.state_dict(), '../data/models/model8.pt')\n",
    "        elif model_num == 9: # summed overview, title, companies and poster\n",
    "            torch.save(model.state_dict(), '../data/models/model9.pt')\n",
    "    \n",
    "        val_acc = total_acc_val/len(t_val_loader)\n",
    "        val_loss = total_loss_val/len(t_val_loader)\n",
    "        print(f'Epoch {i+1}: train_loss: {train_loss:.4f} train_acc: {train_acc:.4f} | val_loss: {val_loss:.4f} val_acc: {val_acc:.4f}')\n",
    "        model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function returns accuracy and roc auc score of a model on the test set.\n",
    "def test_acc_auc(model,text_test_loader, image_test_loader, criterion,num_feats):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_acc_test = 0\n",
    "    total_loss_test = 0\n",
    "    outputs = []\n",
    "    with torch.no_grad():\n",
    "        for lstm, cnn in zip(text_test_loader, image_test_loader):\n",
    "            cnn_inp, cnn_labels = cnn\n",
    "            cnn_inp = cnn_inp.float()\n",
    "            cnn_inp, cnn_labels = cnn_inp.to(device), cnn_labels.to(device)\n",
    "            if num_feats == 2:\n",
    "                lstm_inp, lstm_labels = lstm\n",
    "                lstm_inp = lstm_inp.float()\n",
    "                \n",
    "                lstm_inp, lstm_labels = lstm_inp.to(device), lstm_labels.to(device)\n",
    "                \n",
    "                out = model(lstm_inp, cnn_inp)\n",
    "            elif num_feats == 3:\n",
    "                lstm_inp, title_inp, lstm_labels = lstm\n",
    "                lstm_inp = lstm_inp.float()\n",
    "                title_inp = title_inp.float()\n",
    "                lstm_inp, title_inp,lstm_labels = lstm_inp.to(device),title_inp.to(device), lstm_labels.to(device)\n",
    "                out = model(lstm_inp, cnn_inp,title_inp)\n",
    "            else:\n",
    "                lstm_inp, title_inp, company_inp, lstm_labels = lstm\n",
    "                lstm_inp = lstm_inp.float()\n",
    "                title_inp = title_inp.float()\n",
    "                company_inp = company_inp.float()                \n",
    "                lstm_inp, title_inp, company_inp, lstm_labels = lstm_inp.to(device), title_inp.to(device), company_inp.to(device),  lstm_labels.to(device)                \n",
    "                out = model(lstm_inp, cnn_inp, title_inp, company_inp)\n",
    "                \n",
    "            \n",
    "                  \n",
    "            outputs += list(out.cpu().data.numpy())\n",
    "            loss = criterion(out.squeeze(), lstm_labels.float())\n",
    "            acc = torch.abs(out.squeeze() - lstm_labels.float()).view(-1)\n",
    "            acc = (1. - acc.sum() / acc.size()[0])\n",
    "            total_acc_test += acc\n",
    "            total_loss_test += loss.item()\n",
    "    \n",
    "    acc_test = total_acc_test/len(title_test_loader)\n",
    "    loss_test = total_loss_test/len(title_test_loader)        \n",
    "    \n",
    "    np_out = np.array(outputs)\n",
    "    y_pred = np.zeros(np_out.shape)\n",
    "    y_pred[np_out>0.5]= 1 # threshold to assign a label is a probability of 0.5.\n",
    "    y_pred = np.array(y_pred)    \n",
    "    preds = np.transpose(y_pred)\n",
    "    labels = np.transpose(test_labels)\n",
    "    roc_auc = roc_auc_score(labels, preds) # calculating ROC-AUC score\n",
    "    print(f'acc: {acc_test:.4f}, loss: {loss_test:.4f}, roc auc: {roc_auc}')\n",
    "    return acc_test, roc_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Title and image as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenated title and poster input to model\n",
    "model_title = CNN_LSTM2sc(vocab_size, word2vec_matrix, hidden_dim, n_layers, output_size)\n",
    "model_title.to(device)\n",
    "train_val(model_title,epochs, title_train_loader, img_train_loader,criterion, title_val_loader, img_val_loader, 2,0)\n",
    "test_acc_auc(model_title, title_test_loader, img_test_loader, criterion,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc_auc(model_title, title_test_loader, img_test_loader, criterion,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and test model with summed title and poster as input\n",
    "model_title_sum = CNN_LSTM2sc(vocab_size, word2vec_matrix, hidden_dim, n_layers, output_size, concat=False)\n",
    "model_title_sum.to(device)\n",
    "train_val(model_title_sum,epochs, title_train_loader, img_train_loader,criterion, title_val_loader, img_val_loader, 2, 5)\n",
    "test_acc_auc(model_title_sum, title_test_loader, img_test_loader, criterion,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview and image as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenated features\n",
    "model_overview = CNN_LSTM2sc(vocab_size, word2vec_matrix, hidden_dim, n_layers, output_size)\n",
    "model_overview.to(device)\n",
    "train_val(model_overview, epochs, overview_train_loader, img_train_loader,criterion, overview_val_loader, img_val_loader, 2, 1)\n",
    "test_acc_auc(model_overview, overview_test_loader, img_test_loader, criterion,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summed features\n",
    "model_overview_sum = CNN_LSTM2sc(vocab_size, word2vec_matrix, hidden_dim, n_layers, output_size, concat=False)\n",
    "model_overview_sum.to(device)\n",
    "train_val(model_overview_sum, epochs, overview_train_loader, img_train_loader,criterion, overview_val_loader, img_val_loader, 2, 4)\n",
    "test_acc_auc(model_overview_sum, overview_test_loader, img_test_loader, criterion,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Title, image and poster as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenated\n",
    "model3 = CNN_LSTM3sc(vocab_size, word2vec_matrix, hidden_dim, n_layers, output_size)\n",
    "model3.to(device)\n",
    "train_val(model3, epochs, three_feats_train_loader, img_train_loader,criterion, three_feats_val_loader, img_val_loader, 3, 2)\n",
    "test_acc_auc(model3, three_feats_test_loader, img_test_loader, criterion,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summed features\n",
    "model3_sum = CNN_LSTM3sc(vocab_size, word2vec_matrix, hidden_dim, n_layers, output_size, concat=False)\n",
    "model3_sum.to(device)\n",
    "train_val(model3_sum, epochs, three_feats_train_loader, img_train_loader,criterion, three_feats_val_loader, img_val_loader, 3, 7)\n",
    "test_acc_auc(model3_sum, three_feats_test_loader, img_test_loader, criterion,3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Production companies and Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenated features\n",
    "model_comp = CNN_LSTM2(vocab_size, word2vec_matrix, hidden_dim, n_layers, output_size)\n",
    "model_comp.to(device)\n",
    "train_val(model_comp, epochs, company_train_loader, img_train_loader,criterion, company_val_loader, img_val_loader, 2, 3)\n",
    "test_acc_auc(model_comp, company_test_loader, img_test_loader, criterion, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summed features\n",
    "model_comp_sum = CNN_LSTM2(vocab_size, word2vec_matrix, hidden_dim, n_layers, output_size, concat=False)\n",
    "model_comp_sum.to(device)\n",
    "train_val(model_comp_sum, epochs, company_train_loader, img_train_loader,criterion, company_val_loader, img_val_loader, 2, 8)\n",
    "test_acc_auc(model_comp_sum, company_test_loader, img_test_loader, criterion, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview, title, production companies & poster as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = CNN_LSTM4sc(vocab_size, word2vec_matrix, hidden_dim, n_layers, output_size)\n",
    "model4.to(device)\n",
    "train_val(model4, epochs, four_feats_train_loader, img_train_loader, criterion, four_feats_val_loader, img_val_loader, 4, 4)\n",
    "test_acc_auc(model4, four_feats_test_loader, img_test_loader, criterion, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4_sum = CNN_LSTM4sc(vocab_size, word2vec_matrix, hidden_dim, n_layers, output_size, concat=False)\n",
    "model4_sum.to(device)\n",
    "train_val(model4_sum, epochs, four_feats_train_loader, img_train_loader, criterion, four_feats_val_loader, img_val_loader, 4, 9)\n",
    "test_acc_auc(model4_sum, four_feats_test_loader, img_test_loader, criterion, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#action, comedy, crime, drama the lowest, horror, romance, thriller\n",
    "# cooccurence of genres plot\n",
    "l = df.genres.values\n",
    "\n",
    "c = [list(combinations(i,2)) for i in l]\n",
    "a = list(itertools.chain.from_iterable((i, i[::-1]) for c_ in c for i in c_))\n",
    "dft = pd.DataFrame(a)\n",
    "aba=pd.pivot_table(dft, index=0, columns=1, aggfunc='size', fill_value=0)\n",
    "sns.heatmap(aba,cmap=\"YlGn\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
