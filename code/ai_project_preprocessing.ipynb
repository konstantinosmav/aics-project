{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# importing modules I will use\n",
    "import pandas as pd\n",
    "import json\n",
    "from collections import defaultdict, Counter\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import json\n",
    "from flask import Flask, request\n",
    "from jinja2 import Environment\n",
    "from urllib.request import Request, urlopen\n",
    "import os\n",
    "import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import numpy as np\n",
    "# API key I use to download the movie posters from TMDB\n",
    "api_key = '39329068bc1de1536d231b6b49c9ff50'\n",
    "import numpy as np\n",
    "import ast\n",
    "import re\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from gensim.models import KeyedVectors\n",
    "import string\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchtext.data import Field, BucketIterator, Iterator, TabularDataset\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "device = torch.device('cuda:2') #if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mostly Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gusmavko@GU.GU.SE/.local/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3147: DtypeWarning: Columns (10) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# loading the dataset.\n",
    "\n",
    "df=pd.read_csv('movies_metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['adult', 'belongs_to_collection', 'budget', 'genres', 'homepage', 'id',\n",
       "       'imdb_id', 'original_language', 'original_title', 'overview',\n",
       "       'popularity', 'poster_path', 'production_companies',\n",
       "       'production_countries', 'release_date', 'revenue', 'runtime',\n",
       "       'spoken_languages', 'status', 'tagline', 'title', 'video',\n",
       "       'vote_average', 'vote_count'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# other features to use\n",
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_title</th>\n",
       "      <th>overview</th>\n",
       "      <th>id</th>\n",
       "      <th>genres</th>\n",
       "      <th>original_language</th>\n",
       "      <th>production_companies</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Toy Story</td>\n",
       "      <td>Led by Woody, Andy's toys live happily in his ...</td>\n",
       "      <td>862</td>\n",
       "      <td>[{'id': 16, 'name': 'Animation'}, {'id': 35, '...</td>\n",
       "      <td>en</td>\n",
       "      <td>[{'name': 'Pixar Animation Studios', 'id': 3}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jumanji</td>\n",
       "      <td>When siblings Judy and Peter discover an encha...</td>\n",
       "      <td>8844</td>\n",
       "      <td>[{'id': 12, 'name': 'Adventure'}, {'id': 14, '...</td>\n",
       "      <td>en</td>\n",
       "      <td>[{'name': 'TriStar Pictures', 'id': 559}, {'na...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Grumpier Old Men</td>\n",
       "      <td>A family wedding reignites the ancient feud be...</td>\n",
       "      <td>15602</td>\n",
       "      <td>[{'id': 10749, 'name': 'Romance'}, {'id': 35, ...</td>\n",
       "      <td>en</td>\n",
       "      <td>[{'name': 'Warner Bros.', 'id': 6194}, {'name'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Waiting to Exhale</td>\n",
       "      <td>Cheated on, mistreated and stepped on, the wom...</td>\n",
       "      <td>31357</td>\n",
       "      <td>[{'id': 35, 'name': 'Comedy'}, {'id': 18, 'nam...</td>\n",
       "      <td>en</td>\n",
       "      <td>[{'name': 'Twentieth Century Fox Film Corporat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Father of the Bride Part II</td>\n",
       "      <td>Just when George Banks has recovered from his ...</td>\n",
       "      <td>11862</td>\n",
       "      <td>[{'id': 35, 'name': 'Comedy'}]</td>\n",
       "      <td>en</td>\n",
       "      <td>[{'name': 'Sandollar Productions', 'id': 5842}...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                original_title  \\\n",
       "0                    Toy Story   \n",
       "1                      Jumanji   \n",
       "2             Grumpier Old Men   \n",
       "3            Waiting to Exhale   \n",
       "4  Father of the Bride Part II   \n",
       "\n",
       "                                            overview     id  \\\n",
       "0  Led by Woody, Andy's toys live happily in his ...    862   \n",
       "1  When siblings Judy and Peter discover an encha...   8844   \n",
       "2  A family wedding reignites the ancient feud be...  15602   \n",
       "3  Cheated on, mistreated and stepped on, the wom...  31357   \n",
       "4  Just when George Banks has recovered from his ...  11862   \n",
       "\n",
       "                                              genres original_language  \\\n",
       "0  [{'id': 16, 'name': 'Animation'}, {'id': 35, '...                en   \n",
       "1  [{'id': 12, 'name': 'Adventure'}, {'id': 14, '...                en   \n",
       "2  [{'id': 10749, 'name': 'Romance'}, {'id': 35, ...                en   \n",
       "3  [{'id': 35, 'name': 'Comedy'}, {'id': 18, 'nam...                en   \n",
       "4                     [{'id': 35, 'name': 'Comedy'}]                en   \n",
       "\n",
       "                                production_companies  \n",
       "0     [{'name': 'Pixar Animation Studios', 'id': 3}]  \n",
       "1  [{'name': 'TriStar Pictures', 'id': 559}, {'na...  \n",
       "2  [{'name': 'Warner Bros.', 'id': 6194}, {'name'...  \n",
       "3  [{'name': 'Twentieth Century Fox Film Corporat...  \n",
       "4  [{'name': 'Sandollar Productions', 'id': 5842}...  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we want to only keep the columns that are relevant for our task and drop the rest.\n",
    "\n",
    "df = df[['original_title','overview','id','genres','original_language','production_companies']]\n",
    "df_copy = df.copy()\n",
    "df_copy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of genres that do not really make sense and appear only once so I decided to remove them.\n",
    "\n",
    "weird_genres = ['TV Movie', 'Carousel Productions', 'Vision View Entertainment',\n",
    " 'Telescene Film Group Productions', 'Aniplex', 'GoHands',\n",
    " 'BROSTA TV', 'Mardock Scramble Production Committee', 'Sentai Filmworks',\n",
    " 'Odyssey Media', 'Pulser Productions', 'Rogue State', 'The Cartel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function cleans up the column containing the movie genres, which has a really weird formatting.\n",
    "\n",
    "def clean_genres(df):\n",
    "    list_of_genres = []\n",
    "    for i,row in df['genres'].iteritems():\n",
    "        row = ast.literal_eval(row)\n",
    "        row_list = []\n",
    "        for dic in row:\n",
    "            if dic['name'] not in weird_genres:\n",
    "                row_list.append(dic['name'])\n",
    "        list_of_genres.append(row_list)\n",
    "    return list_of_genres\n",
    "        \n",
    "df['genres'] = clean_genres(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['original_title', 'overview', 'id', 'genres', 'poster_paths'], dtype='object')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42306, 6)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dropping occurrences that appear only once and NaNs\n",
    "\n",
    "df = df[df['genres'].map(lambda d: len(d)) > 0].dropna()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing all rows of non English movies\n",
    "\n",
    "def isEnglish(s):\n",
    "    return s.isascii()\n",
    "def remove_noneng_titles(df):\n",
    "    noneng = []\n",
    "    for title in df['original_title'].tolist():\n",
    "        if not isEnglish(title):\n",
    "            noneng.append(title)\n",
    "    df = df[~df['original_title'].isin(noneng)]\n",
    "    return df\n",
    "df = remove_noneng_titles(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This function plots the distribution of movie genres in the dataset. \n",
    "\n",
    "def plot_distribution(df):\n",
    "    genres = df.genres.values\n",
    "    flattened_genres = [item for sublist in genres for item in sublist]\n",
    "    count_of_genres = Counter(flattened_genres)    \n",
    "    fig, ax = plt.subplots()\n",
    "    plt.bar(count_of_genres.keys(), count_of_genres.values())\n",
    "    plt.title(\"Genre Distribution\")\n",
    "    plt.ylabel('Genre Frequency')\n",
    "    plt.xlabel('Genres')\n",
    "    # Rotate 45 degrees \n",
    "    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha=\"right\" )\n",
    "    plt.gcf().subplots_adjust(bottom=0.15)\n",
    "    plt.show()\n",
    "    \n",
    "plot_distribution(df)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function uses the TMDB API to retrieve a url of every movie poster and then it downloads it locally.\n",
    "def get_data(movie_id):\n",
    "    # I use try - except to avoid my function crashing from potential errors (e.g. in the case that there is no poster in the json dictionary)\n",
    "    try:\n",
    "        url = f\"https://api.themoviedb.org/3/movie/{movie_id}?api_key={api_key}\"\n",
    "     \n",
    "        request = Request(url)\n",
    "        response = urlopen(request)\n",
    "        data = response.read()\n",
    "        poster_path = json.loads(data)['poster_path']        \n",
    "        init_url = 'https://image.tmdb.org/t/p/w500'\n",
    "        image_url = init_url + poster_path     \n",
    "        r = requests.get(image_url)\n",
    "        name = poster_path.replace('/', '_') \n",
    "        filename = f\"poster{name}\"\n",
    "        pa = os.path.join('/home/gusmavko@GU.GU.SE/aics-project/data/images', filename)        \n",
    "        # preferred for \"binary\" filetypes, like poster images\n",
    "        with open(pa,'wb') as w:\n",
    "            w.write(r.content)       \n",
    "        return pa\n",
    "    except Exception:\n",
    "        return 'Error'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function iterates through the dataframe id column and downloads the posters for all movies that have an available poster url.\n",
    "def add_poster(df): \n",
    "    poster_paths = []\n",
    "    for movie_id in tqdm.tqdm(df['id'].tolist()):\n",
    "        try:\n",
    "            poster_paths.append(get_data(movie_id))\n",
    "            print(movie_id)\n",
    "        except Exception as e:\n",
    "            print('[ERROR]', str(e))\n",
    "            poster_paths.append(\"API Error\")\n",
    "    \n",
    "    return poster_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This takes so long to run, I have downloaded them locally so I can perhaps just upload them on drive if somebody does not want to run this.\n",
    "\n",
    "list_of_posters = add_poster(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'list_of_posters' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-fe4f76dbb3e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposter_paths\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0munidentifiable_image_file\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinalize_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-27-fe4f76dbb3e4>\u001b[0m in \u001b[0;36mfinalize_df\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# There is something wrong with this image, it can't be opened so I decided to just drop it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0munidentifiable_image_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/home/gusmavko@GU.GU.SE/aics-project/data/images/poster_b15FrCKeWVH62Sn3o69ZXZi3bBi.jpg'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'poster_paths'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist_of_posters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposter_paths\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"API Error\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposter_paths\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'list_of_posters' is not defined"
     ]
    }
   ],
   "source": [
    "# I also dropped all rows that did not have a poster or movies that caused an error. \n",
    "    \n",
    "def finalize_df(df):\n",
    "    # There is something wrong with this image, it can't be opened so I decided to just drop it.\n",
    "    unidentifiable_image_file = '/home/gusmavko@GU.GU.SE/aics-project/data/images/poster_b15FrCKeWVH62Sn3o69ZXZi3bBi.jpg'\n",
    "    df['poster_paths'] = list_of_posters  \n",
    "    df = df[df.poster_paths != \"API Error\"]\n",
    "    df = df[df.poster_paths != \"Error\"]\n",
    "    df = df[df.poster_paths != unidentifiable_image_file]\n",
    "    return df\n",
    "df = finalize_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv file to be able to load it easily later\n",
    "df.to_csv(\"/home/gusmavko@GU.GU.SE/aics-project/data/dataset.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28466, 24)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df = pd.read_csv(\"/home/gusmavko@GU.GU.SE/aics-project/data/dataset.csv\")\n",
    "\n",
    "#df = remove_noneng_titles(df)\n",
    "np.random.seed(10)\n",
    "#\n",
    "remove_n = 17000\n",
    "#df = pd.DataFrame({\"a\":[1,2,3,4], \"b\":[5,6,7,8]})\n",
    "drop_indices = np.random.choice(df.index, remove_n, replace=False)\n",
    "df = df.drop(drop_indices)\n",
    "#df_subset.shape[0]\n",
    "df.shape\n",
    "df.to_csv('\"/home/gusmavko@GU.GU.SE/aics-project/data/half_data.csv\",index=False')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train, val = train_test_split(train, test_size=0.1, random_state = 42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12567, 5)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#val.to_csv(\"/home/gusmavko@GU.GU.SE/aics-project/data/val_half.csv\",index=False)\n",
    "#test.to_csv(\"/home/gusmavko@GU.GU.SE/aics-project/data/test_half.csv\",index=False)\n",
    "#train.to_csv(\"/home/gusmavko@GU.GU.SE/aics-project/data/train_half.csv\",index=False)\n",
    "\n",
    "val_df = pd.read_csv('/home/gusmavko@GU.GU.SE/aics-project/data/val_half.csv')\n",
    "test_df = pd.read_csv('/home/gusmavko@GU.GU.SE/aics-project/data/test_half.csv')\n",
    "train_df = pd.read_csv('/home/gusmavko@GU.GU.SE/aics-project/data/train_half.csv')\n",
    "\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function makes a tensor out of an image array and saves it to the gpu.\n",
    "def create_img_tensor(img_path, dimensions):\n",
    "    try:\n",
    "        img=Image.open(img_path)\n",
    "        res_img = img.resize(dimensions).convert('RGB')\n",
    "        img_array = np.array(res_img)\n",
    "        img_tensor = torch.tensor(img_array)#.to(device) # commented it out the saving to the gpu server because it caused a lot of memory errors when experimenting with the model\n",
    "    \n",
    "        return img_tensor\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function iterates through the dataframe and creates tensor representations of all images\n",
    "def get_tensors(df,dimensions): # dimensions should be a tuple containing (width, height) e.g. (100,100)\n",
    "    \n",
    "    list_imgs = [] \n",
    "    for i, row in df.iterrows():\n",
    "        img = create_img_tensor(row['poster_paths'], dimensions)\n",
    "        if img is not None:\n",
    "            list_imgs.append(img)\n",
    "        else:\n",
    "            df = df.drop(i)            \n",
    "    df_tensors = torch.stack(list_imgs)\n",
    "    return df_tensors#.to(device)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tensor of tensors for validation set\n",
    "val_tensors = get_tensors(val_df,(100,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('/home/gusmavko@GU.GU.SE/aics-project/data/val_tensors','wb') as f: pickle.dump(val_tensors, f)\n",
    "\n",
    "torch.save(val_tensors, '/home/gusmavko@GU.GU.SE/aics-project/data/val_half_tensors.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tensor of tensors for test set\n",
    "test_tensors = get_tensors(test_df,(100,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('/home/gusmavko@GU.GU.SE/aics-project/data/test_tensors','wb') as f: pickle.dump(test_tensors, f)\n",
    "torch.save(test_tensors,'/home/gusmavko@GU.GU.SE/aics-project/data/test_half_tensors.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tensor of tensors for train set\n",
    "train_tensors = get_tensors(train_df,(100,100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('/home/gusmavko@GU.GU.SE/aics-project/data/train_tensors','wb') as f: pickle.dump(train_tensors, f)\n",
    "torch.save(train_tensors,'/home/gusmavko@GU.GU.SE/aics-project/data/train_half_tensors.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12567, 100, 100, 3])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tensors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have the tensors saved locally, I can upload them on my repo too\n",
    "# I usually start things from this point\n",
    "\n",
    "val_tensors = torch.load('/home/gusmavko@GU.GU.SE/aics-project/data/val_half_tensors.pt')\n",
    "test_tensors = torch.load('/home/gusmavko@GU.GU.SE/aics-project/data/test_half_tensors.pt')\n",
    "train_tensors = torch.load('/home/gusmavko@GU.GU.SE/aics-project/data/train_half_tensors.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/gusmavko@GU.GU.SE/aics-project/data/dataset.csv')\n",
    "\n",
    "train_df = pd.read_csv(\"/home/gusmavko@GU.GU.SE/aics-project/data/train_dataset.csv\")\n",
    "test_df = pd.read_csv(\"/home/gusmavko@GU.GU.SE/aics-project/data/test_dataset.csv\")\n",
    "val_df = pd.read_csv(\"/home/gusmavko@GU.GU.SE/aics-project/data/val_dataset.csv\")\n",
    "\n",
    "df['genres']=df['genres'].apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "train_df['genres']=train_df['genres'].apply(lambda x: ast.literal_eval(x))\n",
    "test_df['genres']=test_df['genres'].apply(lambda x: ast.literal_eval(x))\n",
    "val_df['genres']=val_df['genres'].apply(lambda x: ast.literal_eval(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "# We need to pass a list of lists of the labels to get a one-hot encoding for the train, the test and the validation dataset.\n",
    "\n",
    "# Fitting the multilabel binarizer to the labels available.\n",
    "#mlb.fit(df['genres'].tolist())\n",
    "\n",
    "train_labels = mlb.fit_transform(train_df['genres'].tolist())\n",
    "test_labels = mlb.fit_transform(test_df['genres'].tolist())\n",
    "val_labels = mlb.fit_transform(val_df['genres'].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This function creates a dictionary that has the number of movie labels as a key \n",
    "# and the number of occurrences of this number of labels as a value.\n",
    "\n",
    "def plot_num_labels_movies(genre_lists):\n",
    "    dic = {}\n",
    "    for genre_list in genre_lists:\n",
    "        if len(genre_list) in dic:\n",
    "            dic[len(genre_list)] += 1\n",
    "            \n",
    "        else:\n",
    "            dic[len(genre_list)] = 1\n",
    "    return dic \n",
    "\n",
    "plot_dic = plot_num_labels_movies(df.genres.tolist())\n",
    "plt.bar(*zip(*plot_dic.items()))\n",
    "plt.title('Occurrences of different number of labels in the dataset')\n",
    "plt.xlabel('Number of labels')\n",
    "plt.ylabel('Number of occurrences')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "swords = stopwords.words('english')\n",
    "reg_ex_tokenizer = RegexpTokenizer(r'\\w+')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "#stemmer = Porter\n",
    "\n",
    "# Thought about using different cleaning up methods for titles and summary.\n",
    "def clean(text,is_for_summary=True):\n",
    "    # removing punctuation and digits\n",
    "    text = ''.join([c for c in text if c not in string.punctuation and not c.isdigit()])\n",
    "    # tokenizing with the RegexpTokenizer and lowercasing the text\n",
    "    text = reg_ex_tokenizer.tokenize(text.lower())\n",
    "    # removing stop words if cleaning is applied on the overview - if we clean up the titles we keep the stop words since certain titles contain solely stop words(e.g. 'Who am I?').\n",
    "    # maybe I should filter out stop words in titles too and only keep them when there are no other words.\n",
    "    if is_for_summary:        \n",
    "        text = [w for w in text if w not in swords]\n",
    "    # lemmatizing the words with the WordNetLemmatizer and using join to put them back into a string\n",
    "    text = ' '.join([lemmatizer.lemmatize(w) for w in text])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating new column consisting of both the title and the overview, which I will use when I want to \n",
    "#train a model with title, overview and image input.\n",
    "train_df['title_and_overview'] = train_df['original_title'] + ' ' + train_df['overview']\n",
    "test_df['title_and_overview'] = test_df['original_title'] + ' ' + test_df['overview']\n",
    "val_df['title_and_overview'] = val_df['original_title'] + ' ' + val_df['overview']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning up the orignal dataframe's overview and original_title column.\n",
    "df['overview'] = df['overview'].apply(lambda x: clean(x))\n",
    "df['original_title'] = df['original_title'].apply(lambda x: clean(x))\n",
    "\n",
    "# Cleaning up the overview column.\n",
    "train_df['overview'] = train_df['overview'].apply(lambda x: clean(x))\n",
    "test_df['overview'] = test_df['overview'].apply(lambda x: clean(x))\n",
    "val_df['overview'] = val_df['overview'].apply(lambda x: clean(x))\n",
    "\n",
    "# Cleaning up the original title column.\n",
    "train_df['original_title'] = train_df['original_title'].apply(lambda x: clean(x,False))\n",
    "test_df['original_title'] = test_df['original_title'].apply(lambda x: clean(x,False))\n",
    "val_df['original_title'] = val_df['original_title'].apply(lambda x: clean(x,False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'boy michael jackson king even waihau bay new zealand meet boy yearold life farm gran goat younger brother rocky think magic power shortly gran leaf week boy father alamein appears blue imagined heroic version father absence boy come face face real versionan incompetent hoodlum returned find bag money buried year goat enters'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating new column consisting of both the title and the overview, which I will use when I want to \n",
    "#train a model with title, overview and image input.\n",
    "train_df['title_and_overview'] = train_df['original_title'] + ' ' + train_df['overview']\n",
    "test_df['title_and_overview'] = test_df['original_title'] + ' ' + test_df['overview']\n",
    "val_df['title_and_overview'] = val_df['original_title'] + ' ' + val_df['overview']\n",
    "df['title_and_overview'] = df['original_title'] + ' ' + df['overview']\n",
    "\n",
    "train_df['title_and_overview'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def trim_string(x):\n",
    "    x = x.split()\n",
    "    x = x[:400]\n",
    "    x = ' '.join(x)\n",
    "    return x\n",
    "# Cleaning up the original title column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['title_and_overview'] = train_df['title_and_overview'].apply(trim_string)\n",
    "test_df['title_and_overview'] = test_df['title_and_overview'].apply(trim_string)\n",
    "val_df['title_and_overview'] = val_df['title_and_overview'].apply(trim_string)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaned-up original_title and overview datadrame version is saved to csv files.\n",
    "\n",
    "#train_df.to_csv('/home/gusmavko@GU.GU.SE/aics-project/data/train_ds.csv',index=False)\n",
    "#test_df.to_csv('/home/gusmavko@GU.GU.SE/aics-project/data/test_ds.csv',index=False)\n",
    "#val_df.to_csv('/home/gusmavko@GU.GU.SE/aics-project/data/val_ds.csv',index=False)  \n",
    "#df.to_csv('/home/gusmavko@GU.GU.SE/aics-project/data/df_ds.csv',index=False)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reloading the cleaned up csv files as dataframes.\n",
    "train_df = pd.read_csv('/home/gusmavko@GU.GU.SE/aics-project/data/train_ds.csv')\n",
    "test_df = pd.read_csv('/home/gusmavko@GU.GU.SE/aics-project/data/test_ds.csv')\n",
    "val_df = pd.read_csv('/home/gusmavko@GU.GU.SE/aics-project/data/val_ds.csv')\n",
    "df = pd.read_csv('/home/gusmavko@GU.GU.SE/aics-project/data/df_ds.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the two columns containing textual data into one to cover all possible occurrences for words.\n",
    "df_tit_overview = df[['original_title','overview']] \n",
    "df_vocab=df_tit_overview.stack().reset_index()\n",
    "# Getting length of sequences for padding\n",
    "max_len_for_padding = df_vocab[0].map(len).max()\n",
    "max_len_title_overview = df['title_and_overview'].map(len).max()\n",
    "embedding_dim = 300\n",
    "tokenizer = Tokenizer(num_words=50000, lower=True)\n",
    "tokenizer.fit_on_texts(df_vocab[0].values)\n",
    "# Creating a word to integer dictionary for my vocab.\n",
    "word_index = tokenizer.word_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gusmavko@GU.GU.SE/.local/lib/python3.7/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "def embed_vocab(pretrained_dict, vocab, emb_size):\n",
    "    # creating a dictionary with keys the words and values the pretrained word vectors. \n",
    "    embeddings_index = {}\n",
    "    for w in pretrained_dict.wv.vocab:\n",
    "        embeddings_index[w] = pretrained_dict.word_vec(w)     \n",
    "    # instantiating matrix with shape (vocab + 1, 300)\n",
    "    embedding_matrix = 1 * np.random.randn(len(vocab)+1, emb_size)\n",
    "    # looking up the words in my vocab\n",
    "    for word, i in vocab.items():\n",
    "        i-=1\n",
    "        # getting the pretrained vector for the corresponding word\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # adding the vector to the matrix\n",
    "            embedding_matrix[i] = embedding_vector                      \n",
    "    del(embeddings_index)\n",
    "        \n",
    "    return embedding_matrix\n",
    "\n",
    "# need to download the pretrained vectors file\n",
    "\n",
    "w2v_dic = KeyedVectors.load_word2vec_format(\"/home/gusmavko@GU.GU.SE/aics-project/data/GoogleNews-vectors-negative300.bin\", binary=True)\n",
    "word2vec_matrix = embed_vocab(w2v_dic, word_index, 300)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train overview shape:(24807, 829), Test overview shape:(6892, 829), Validation overview shape:(2757, 829)\n"
     ]
    }
   ],
   "source": [
    "# Transforming each word token from the textual data into the corresponding index. \n",
    "# TODO maybe when padding do I need to pad the titles to the same length as the overview \n",
    "\n",
    "X_text_train = tokenizer.texts_to_sequences(train_df['overview'].values)\n",
    "X_text_train = pad_sequences(X_text_train, maxlen=max_len_for_padding)\n",
    "\n",
    "X_text_test = tokenizer.texts_to_sequences(test_df['overview'].values)\n",
    "X_text_test = pad_sequences(X_text_test, maxlen=max_len_for_padding)\n",
    "\n",
    "X_text_val = tokenizer.texts_to_sequences(val_df['overview'].values)\n",
    "X_text_val = pad_sequences(X_text_val, maxlen=max_len_for_padding)\n",
    "\n",
    "print(f'Train overview shape:{X_text_train.shape}, Test overview shape:{X_text_test.shape}, Validation overview shape:{X_text_val.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train title shape:(24807, 92), Test overview shape:(6892, 92), Validation overview shape:(2757, 92)\n"
     ]
    }
   ],
   "source": [
    "# Tranforming the titles to integer representations.\n",
    "\n",
    "max_len_title = df['original_title'].map(len).max()\n",
    "\n",
    "# train title\n",
    "\n",
    "X_title_train = tokenizer.texts_to_sequences(train_df['original_title'].values)\n",
    "X_title_train = pad_sequences(X_title_train,maxlen=max_len_title)\n",
    "# test title\n",
    "\n",
    "X_title_test = tokenizer.texts_to_sequences(test_df['original_title'].values)\n",
    "X_title_test = pad_sequences(X_title_test,maxlen=max_len_title)\n",
    "\n",
    "# val title\n",
    "X_title_val = tokenizer.texts_to_sequences(val_df['original_title'].values)\n",
    "X_title_val = pad_sequences(X_title_val,maxlen=max_len_title)\n",
    "\n",
    "print(f'Train title shape:{X_title_train.shape}, Test overview shape:{X_title_test.shape}, Validation overview shape:{X_title_val.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train title and overview combination shape:(24807, 400),Test title and overview combination shape:(6892, 400),Val title and overview combination shape:(2757, 400)\n"
     ]
    }
   ],
   "source": [
    "# Transforming the textual data containing the title and the overview into integer representations.\n",
    "max_len_title_overview = 400\n",
    "\n",
    "# train title_and_overview transformation\n",
    "X_title_overview_train = tokenizer.texts_to_sequences(train_df['title_and_overview'].values)\n",
    "X_title_overview_train = pad_sequences(X_title_overview_train,maxlen=max_len_title_overview)\n",
    "\n",
    "# test title_and_overview transformation\n",
    "X_title_overview_test = tokenizer.texts_to_sequences(test_df['title_and_overview'].values)\n",
    "X_title_overview_test = pad_sequences(X_title_overview_test,maxlen=max_len_title_overview)\n",
    "\n",
    "# val title_and_overview transformation\n",
    "X_title_overview_val = tokenizer.texts_to_sequences(val_df['title_and_overview'].values)\n",
    "X_title_overview_val = pad_sequences(X_title_overview_val,maxlen=max_len_title_overview)\n",
    "\n",
    "print(f'Train title and overview combination shape:{X_title_overview_train.shape},Test title and overview combination shape:{X_title_overview_test.shape},Val title and overview combination shape:{X_title_overview_val.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the dimensions of the tensors for when passing them in batches to the model.\n",
    "X_img_val = val_tensors.permute(0,3,1,2)\n",
    "\n",
    "X_img_test = test_tensors.permute(0,3,1,2)\n",
    "\n",
    "X_img_train = train_tensors.permute(0,3,1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using TensorDataset to transform the arrays into tensors for my overview textual data\n",
    "text_train_data = TensorDataset(torch.from_numpy(X_text_train), torch.from_numpy(train_labels))\n",
    "img_train_data = TensorDataset(X_img_train, torch.from_numpy(train_labels))\n",
    "\n",
    "\n",
    "text_val_data = TensorDataset(torch.from_numpy(X_text_val), torch.from_numpy(val_labels))\n",
    "img_val_data = TensorDataset(X_img_val, torch.from_numpy(val_labels))\n",
    "text_test_data = TensorDataset(torch.from_numpy(X_text_test), torch.from_numpy(test_labels))\n",
    "img_test_data = TensorDataset(X_img_test, torch.from_numpy(test_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "\n",
    "text_train_loader = DataLoader(text_train_data, batch_size=batch_size)\n",
    "img_train_loader = DataLoader(img_train_data, batch_size=batch_size)\n",
    "\n",
    "text_val_loader = DataLoader(text_val_data, batch_size=batch_size)\n",
    "img_val_loader = DataLoader(img_val_data, batch_size=batch_size)\n",
    "\n",
    "text_test_loader = DataLoader(text_test_data, batch_size=batch_size)\n",
    "img_test_loader = DataLoader(img_test_data, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview and Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building model for movie genre classification using textual and image features.\n",
    "\n",
    "class CNN_LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, weights_matrix, n_hidden, n_layers, n_out):\n",
    "        super(CNN_LSTM, self).__init__()\n",
    "\n",
    "        # LSTM for the text overview\n",
    "        self.vocab_size, self.n_hidden, self.n_out, self.n_layers = vocab_size, n_hidden, n_out, n_layers\n",
    "        num_embeddings, embedding_dim = weights_matrix.shape[0], weights_matrix.shape[1]\n",
    "        self.emb = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.emb.weight.data.copy_(torch.from_numpy(weights_matrix))\n",
    "        self.emb.weight.requires_grad = True\n",
    "        self.lstm = nn.LSTM(embedding_dim, self.n_hidden, self.n_layers, dropout=0.2, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.lstm_fc = nn.Linear(self.n_hidden, 128)\n",
    "        \n",
    "        # CNN for the posters\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3) # Gives me depth of input.\n",
    "        # pooling layers to reduce the image size\n",
    "        self.max_pool1 = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3)\n",
    "        # pooling layers to reduce the image size\n",
    "        self.max_pool2 = nn.MaxPool2d(2)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3)\n",
    "        # pooling layers to reduce the image size\n",
    "        self.max_pool3 = nn.MaxPool2d(2)\n",
    "        self.conv4 = nn.Conv2d(128, 128, 3)\n",
    "        # pooling layers to reduce the image size\n",
    "        self.max_pool4 = nn.MaxPool2d(2)\n",
    "        # to drop the nodes that are below the wanted weights\n",
    "        self.cnn_dropout = nn.Dropout(0.1)\n",
    "        self.cnn_fc = nn.Linear(4*4*128, 512) # Gives me number of features of input.\n",
    "        \n",
    "        # Concat layer for the combined feature space\n",
    "        self.combined_fc1 = nn.Linear(640, 256) # Give me features of input.\n",
    "        self.combined_fc2 = nn.Linear(256, 128)\n",
    "        self.output_fc = nn.Linear(128, n_out)\n",
    "\n",
    "\n",
    "    def forward(self, lstm_inp, cnn_inp):\n",
    "        batch_size = lstm_inp.size(0)\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        lstm_inp = lstm_inp.long()\n",
    "        embeds = self.emb(lstm_inp)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        lstm_out = self.dropout(lstm_out[:, -1])\n",
    "        lstm_out = F.relu(self.lstm_fc(lstm_out))\n",
    "\n",
    "        x = F.relu(self.conv1(cnn_inp))\n",
    "        x = self.max_pool1(x)\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.max_pool2(x)\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.max_pool3(x)\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.max_pool4(x)\n",
    "        #print(x.shape) # that is how I get the correct dimensions for the CNN output that is then passed as input to the linear layer\n",
    "        # need the input to be two dimensional again\n",
    "        x = x.view(-1, 4*4*128)\n",
    "        \n",
    "        x = self.cnn_dropout(x)\n",
    "        cnn_out = F.relu(self.cnn_fc(x))\n",
    "        \n",
    "        #concatenating the cnn output and the lstm output\n",
    "        combined_inp = torch.cat((cnn_out, lstm_out), 1)\n",
    "        # activation function\n",
    "        x_comb = F.relu(self.combined_fc1(combined_inp))\n",
    "        x_comb = F.relu(self.combined_fc2(x_comb))\n",
    "        # sigmoid to get 1 and 0 on the one-hot-encoding formatted label\n",
    "        out = torch.sigmoid(self.output_fc(x_comb))\n",
    "\n",
    "        return out\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device),\n",
    "                          weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device))\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN_LSTM(\n",
      "  (emb): Embedding(72673, 300)\n",
      "  (lstm): LSTM(300, 64, num_layers=2, batch_first=True, dropout=0.2)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (lstm_fc): Linear(in_features=64, out_features=128, bias=True)\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (max_pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (max_pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (max_pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (max_pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (cnn_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (cnn_fc): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (combined_fc1): Linear(in_features=640, out_features=256, bias=True)\n",
      "  (combined_fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (output_fc): Linear(in_features=128, out_features=19, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(word_index)+1\n",
    "output_size = train_labels.shape[1]\n",
    "embedding_dim = 300\n",
    "hidden_dim = 64\n",
    "n_layers = 2\n",
    "\n",
    "model = CNN_LSTM(vocab_size, word2vec_matrix, hidden_dim, n_layers, output_size)\n",
    "model.to(device)\n",
    "print(model)\n",
    "\n",
    "lr=0.001\n",
    "# criterion = nn.MultiLabelSoftMarginLoss()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done training tensor(0.8251, device='cuda:1')\n",
      "Saving model...\n",
      "Epoch 1: train_loss: 0.3006 train_acc: 0.8251 | val_loss: 0.2835 val_acc: 0.8230\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-e5b020c741ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Epoch {i+1}: train_loss: {train_loss:.4f} train_acc: {train_acc:.4f} | val_loss: {val_loss:.4f} val_acc: {val_acc:.4f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib64/python3.7/site-packages/torch/cuda/memory.py\u001b[0m in \u001b[0;36mempty_cache\u001b[0;34m()\u001b[0m\n\u001b[1;32m     85\u001b[0m     \"\"\"\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_emptyCache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory"
     ]
    }
   ],
   "source": [
    "epochs = 20 # try larger number of epochs\n",
    "clip = 5\n",
    "\n",
    "model.train()\n",
    "for i in range(epochs):\n",
    "    total_acc_train = 0\n",
    "    total_loss_train = 0\n",
    "    \n",
    "    for lstm, cnn in zip(text_train_loader, img_train_loader):\n",
    "        lstm_inp,lstm_labels = lstm\n",
    "        lstm_inp = lstm_inp.float()\n",
    "        cnn_inp, cnn_labels = cnn\n",
    "        cnn_inp = cnn_inp.float()\n",
    "        lstm_inp, lstm_labels = lstm_inp.to(device), lstm_labels.to(device)\n",
    "        cnn_inp, cnn_labels = cnn_inp.to(device), cnn_labels.to(device)\n",
    "        model.zero_grad()\n",
    "        output = model(lstm_inp, cnn_inp)\n",
    "        loss = criterion(output.squeeze(), lstm_labels.float())\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            acc = torch.abs(output.squeeze() - lstm_labels.float()).view(-1)\n",
    "            acc = (1. - acc.sum() / acc.size()[0])\n",
    "            total_acc_train += acc\n",
    "            total_loss_train += loss.item()\n",
    "  \n",
    "    train_acc = total_acc_train/len(text_train_loader)\n",
    "    train_loss = total_loss_train/len(text_train_loader)\n",
    "    print('done training',train_acc)\n",
    "    model.eval()\n",
    "    total_acc_val = 0\n",
    "    total_loss_val = 0\n",
    "    with torch.no_grad():\n",
    "        for lstm, cnn in zip(text_val_loader, img_val_loader):\n",
    "            lstm_inp, lstm_labels = lstm\n",
    "            lstm_inp = lstm_inp.float()\n",
    "            cnn_inp, cnn_labels = cnn\n",
    "            cnn_inp = cnn_inp.float()\n",
    "            lstm_inp, lstm_labels = lstm_inp.to(device), lstm_labels.to(device)\n",
    "            cnn_inp, cnn_labels = cnn_inp.to(device), cnn_labels.to(device)\n",
    "            model.zero_grad()\n",
    "            output = model(lstm_inp, cnn_inp)\n",
    "            val_loss = criterion(output.squeeze(), lstm_labels.float())\n",
    "            acc = torch.abs(output.squeeze() - lstm_labels.float()).view(-1)\n",
    "            acc = (1. - acc.sum() / acc.size()[0])\n",
    "            total_acc_val += acc\n",
    "            total_loss_val += val_loss.item()\n",
    "    print(\"Saving model...\") \n",
    "    torch.save(model.state_dict(), '/home/gusmavko@GU.GU.SE/aics-project/data/model_title_img.pt')\n",
    "\n",
    "    val_acc = total_acc_val/len(text_val_loader)\n",
    "    val_loss = total_loss_val/len(text_val_loader)\n",
    "    print(f'Epoch {i+1}: train_loss: {train_loss:.4f} train_acc: {train_acc:.4f} | val_loss: {val_loss:.4f} val_acc: {val_acc:.4f}')\n",
    "    model.train()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt at using title, image, poster as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempting to add title when loading the dataset.\n",
    "# Again, using TensorDataset to transform the arrays into tensors for my textual data.\n",
    "\n",
    "## train dataset\n",
    "#text_train_data = TensorDataset(torch.from_numpy(X_text_train), torch.from_numpy(X_title_train), torch.from_numpy(train_labels))\n",
    "##img_train_data = TensorDataset(X_img_train, torch.from_numpy(train_labels))\n",
    "#\n",
    "## validation dataset\n",
    "#text_val_data = TensorDataset(torch.from_numpy(X_text_val), torch.from_numpy(X_title_val), torch.from_numpy(val_labels))\n",
    "##img_val_data = TensorDataset(X_img_val, torch.from_numpy(val_labels))\n",
    "#\n",
    "## test dataset\n",
    "#text_test_data = TensorDataset(torch.from_numpy(X_text_test), torch.from_numpy(test_labels))\n",
    "#img_test_data = TensorDataset(X_img_test, torch.from_numpy(test_labels))\n",
    "#\n",
    "#batch_size = 40\n",
    "#\n",
    "#text_train_loader = DataLoader(text_train_data, batch_size=batch_size)\n",
    "#img_train_loader = DataLoader(img_train_data, batch_size=batch_size)\n",
    "#\n",
    "#text_val_loader = DataLoader(text_val_data, batch_size=batch_size)\n",
    "#img_val_loader = DataLoader(img_val_data, batch_size=batch_size)\n",
    "#\n",
    "#text_test_loader = DataLoader(text_test_data, batch_size=batch_size)\n",
    "#img_test_loader = DataLoader(img_test_data, batch_size=batch_size)\n",
    "\n",
    "# using TensorDataset to transform the arrays into tensors for my overview textual data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN_LSTM_3(\n",
      "  (emb): Embedding(72673, 300)\n",
      "  (lstm): LSTM(300, 64, num_layers=2, batch_first=True, dropout=0.2)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (lstm_fc): Linear(in_features=64, out_features=128, bias=True)\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (max_pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (max_pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (max_pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (max_pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (cnn_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (cnn_fc): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (combined_fc1): Linear(in_features=640, out_features=256, bias=True)\n",
      "  (combined_fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (output_fc): Linear(in_features=128, out_features=19, bias=True)\n",
      ")\n",
      "cuda:2\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(word_index)+1\n",
    "output_size = train_labels.shape[1]\n",
    "embedding_dim = 300\n",
    "hidden_dim = 64\n",
    "n_layers = 2\n",
    "\n",
    "model = CNN_LSTM_3(vocab_size, word2vec_matrix, hidden_dim, n_layers, output_size)\n",
    "model.to(device)\n",
    "print(model)\n",
    "\n",
    "lr=0.001\n",
    "# criterion = nn.MultiLabelSoftMarginLoss()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building model for movie genre classification using textual and image features.\n",
    "\n",
    "class CNN_LSTM_3(nn.Module):\n",
    "    def __init__(self, vocab_size, weights_matrix, n_hidden, n_layers, n_out):\n",
    "        super(CNN_LSTM_3, self).__init__()\n",
    "\n",
    "        # LSTM for the text overview\n",
    "        self.vocab_size, self.n_hidden, self.n_out, self.n_layers = vocab_size, n_hidden, n_out, n_layers\n",
    "        num_embeddings, embedding_dim = weights_matrix.shape[0], weights_matrix.shape[1]\n",
    "        self.emb = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.emb.weight.data.copy_(torch.from_numpy(weights_matrix))\n",
    "        self.emb.weight.requires_grad = True\n",
    "        self.lstm = nn.LSTM(embedding_dim, self.n_hidden, self.n_layers, dropout=0.2, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.lstm_fc = nn.Linear(self.n_hidden, 128)\n",
    "        \n",
    "        # CNN for the posters\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3) # Gives me depth of input.\n",
    "        # pooling layers to reduce the image size\n",
    "        self.max_pool1 = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3)\n",
    "        # pooling layers to reduce the image size\n",
    "        self.max_pool2 = nn.MaxPool2d(2)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3)\n",
    "        # pooling layers to reduce the image size\n",
    "        self.max_pool3 = nn.MaxPool2d(2)\n",
    "        self.conv4 = nn.Conv2d(128, 128, 3)\n",
    "        # pooling layers to reduce the image size\n",
    "        self.max_pool4 = nn.MaxPool2d(2)\n",
    "        # to drop the nodes that are below the wanted weights\n",
    "        self.cnn_dropout = nn.Dropout(0.1)\n",
    "        self.cnn_fc = nn.Linear(4*4*128, 512) # Gives me number of features of input.\n",
    "        \n",
    "        # Concat layer for the combined feature space\n",
    "        self.combined_fc1 = nn.Linear(640, 256) # Give me features of input.\n",
    "        self.combined_fc2 = nn.Linear(256, 128)\n",
    "        self.output_fc = nn.Linear(128, n_out)\n",
    "\n",
    "\n",
    "    def forward(self, lstm_in, cnn_inp, title_inp):\n",
    "        batch_size = lstm_in.size(0)\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        lstm_in = lstm_in.long()\n",
    "        title_inp = title_inp.long()\n",
    "        lstm_inp = torch.cat((lstm_in,title_inp),dim=1)\n",
    "        embeds = self.emb(lstm_inp)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        lstm_out = self.dropout(lstm_out[:, -1])\n",
    "        lstm_out = F.relu(self.lstm_fc(lstm_out))\n",
    "        \n",
    "        # title input\n",
    "        #title_inp = title_inp.long()\n",
    "        #embeds2 = self.emb(title_inp)\n",
    "        #title_out, hidden = self.lstm(embeds2, hidden)\n",
    "        #title_out = self.dropout(title_out[:,-1])\n",
    "        #title_out = F.relu(self.lstm_fc(title_out))\n",
    "        \n",
    "        #combined_text_inp = torch.cat((title_out, lstm_out), 1)\n",
    "        \n",
    "        x = F.relu(self.conv1(cnn_inp))\n",
    "        x = self.max_pool1(x)\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.max_pool2(x)\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.max_pool3(x)\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.max_pool4(x)\n",
    "        #print(x.shape) # that is how I get the correct dimensions for the CNN output that is then passed as input to the linear layer\n",
    "        # need the input to be two dimensional again\n",
    "        x = x.view(-1, 4*4*128)\n",
    "        \n",
    "        x = self.cnn_dropout(x)\n",
    "        cnn_out = F.relu(self.cnn_fc(x))\n",
    "        \n",
    "        #concatenating the cnn output and the lstm output\n",
    "        combined_inp = torch.cat((cnn_out, lstm_out), 1)\n",
    "        # activation function\n",
    "        x_comb = F.relu(self.combined_fc1(combined_inp))\n",
    "        x_comb = F.relu(self.combined_fc2(x_comb))\n",
    "        # sigmoid to get 1 and 0 on the one-hot-encoding formatted label\n",
    "        out = torch.sigmoid(self.output_fc(x_comb))\n",
    "\n",
    "        return out\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device),\n",
    "                          weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device))\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN_LSTM_3(\n",
      "  (emb): Embedding(72673, 300)\n",
      "  (lstm): LSTM(300, 64, num_layers=2, batch_first=True, dropout=0.2)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (lstm_fc): Linear(in_features=64, out_features=128, bias=True)\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (max_pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (max_pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (max_pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (max_pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (cnn_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (cnn_fc): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (combined_fc1): Linear(in_features=640, out_features=256, bias=True)\n",
      "  (combined_fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (output_fc): Linear(in_features=128, out_features=19, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "vocab_size = len(word_index)+1\n",
    "output_size = train_labels.shape[1]\n",
    "embedding_dim = 300\n",
    "hidden_dim = 64\n",
    "n_layers = 2\n",
    "\n",
    "model = CNN_LSTM_3(vocab_size, word2vec_matrix, hidden_dim, n_layers, output_size)\n",
    "model.to(device)\n",
    "print(model)\n",
    "\n",
    "lr=0.001\n",
    "# criterion = nn.MultiLabelSoftMarginLoss()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3098, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.2928, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.3475, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.3386, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.3130, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.3185, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.3259, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.3549, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.3241, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.3205, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.3666, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.3545, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.2858, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.3169, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.2699, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.3195, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.3311, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.3425, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.3171, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.3166, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.3067, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.3040, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.3056, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.2819, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.2953, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.2599, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.3343, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.3212, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.3208, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.3319, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.3189, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.3219, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.3032, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.3146, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.3091, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.3247, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.3326, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.3107, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.2823, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.3151, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.3138, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.3235, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.3054, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.2647, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.3020, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.2877, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.3144, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.3444, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.3290, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.3055, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.2707, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.3184, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.3028, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.3087, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.3479, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.3327, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.2946, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.3410, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.3037, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.3270, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.3268, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.3225, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.3034, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.3340, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.2947, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.2858, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.3323, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.3051, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.2829, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.3249, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.3093, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.2945, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.2781, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.3027, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.2954, grad_fn=<BinaryCrossEntropyBackward>)\n"
     ]
    }
   ],
   "source": [
    "epochs = 20 # try larger number of epochs\n",
    "clip = 5\n",
    "#model = CNN_LSTM_3(vocab_size, word2vec_matrix, hidden_dim, n_layers, output_size)\n",
    "#model.to(device)\n",
    "model.train()\n",
    "for i in range(epochs):\n",
    "    total_acc_train = 0\n",
    "    total_loss_train = 0\n",
    "    \n",
    "    for lstm, cnn in zip(text_train_loader, img_train_loader):\n",
    "        lstm_inp, title_inp, lstm_labels = lstm\n",
    "        \n",
    "        title_inp = title_inp.float()\n",
    "        lstm_inp = lstm_inp.float()\n",
    "        #print(lstm_inp)\n",
    "        cnn_inp, cnn_labels = cnn\n",
    "        cnn_inp = cnn_inp.float()\n",
    "        #title_inp,title_labels = title_inp.to(device),title_labels.to(device)\n",
    "        lstm_inp, title_inp,lstm_labels = lstm_inp.to(device), title_inp.to(device), lstm_labels.to(device)\n",
    "        cnn_inp, cnn_labels = cnn_inp.to(device), cnn_labels.to(device)\n",
    "        model.zero_grad()\n",
    "        output = model(lstm_inp, cnn_inp,title_inp)\n",
    "        #print(output)\n",
    "        loss = criterion(output.squeeze(), lstm_labels.float())\n",
    "        print(loss)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            acc = torch.abs(output.squeeze() - lstm_labels.float()).view(-1)\n",
    "            acc = (1. - acc.sum() / acc.size()[0])\n",
    "            total_acc_train += acc\n",
    "            total_loss_train += loss.item()\n",
    "  \n",
    "    train_acc = total_acc_train/len(text_train_loader)\n",
    "    train_loss = total_loss_train/len(text_train_loader)\n",
    "    print('done training',train_acc)\n",
    "    model.eval()\n",
    "    total_acc_val = 0\n",
    "    total_loss_val = 0\n",
    "    with torch.no_grad():\n",
    "        for lstm, cnn in zip(text_val_loader, img_val_loader):\n",
    "            lstm_inp, title_inp, lstm_labels = lstm\n",
    "            lstm_inp = lstm_inp.float()\n",
    "            title_inp = title_inp.float()\n",
    "            cnn_inp, cnn_labels = cnn\n",
    "            cnn_inp = cnn_inp.float()\n",
    "            lstm_inp, title_inp, lstm_labels = lstm_inp.to(device),title_inp.tο(device), lstm_labels.to(device)\n",
    "            cnn_inp, cnn_labels = cnn_inp.to(device), cnn_labels.to(device)\n",
    "            model.zero_grad()\n",
    "            output = model(lstm_inp, cnn_inp,title_inp)\n",
    "            val_loss = criterion(output.squeeze(), lstm_labels.float())\n",
    "            acc = torch.abs(output.squeeze() - lstm_labels.float()).view(-1)\n",
    "            acc = (1. - acc.sum() / acc.size()[0])\n",
    "            total_acc_val += acc\n",
    "            total_loss_val += val_loss.item()\n",
    "    print(\"Saving model...\") \n",
    "    torch.save(model.state_dict(), '/home/gusmavko@GU.GU.SE/aics-project/data/model_title_overview_img.pt')\n",
    "#\n",
    "    val_acc = total_acc_val/len(text_val_loader)\n",
    "    val_loss = total_loss_val/len(text_val_loader)\n",
    "    print(f'Epoch {i+1}: train_loss: {train_loss:.4f} train_acc: {train_acc:.4f} | val_loss: {val_loss:.4f} val_acc: {val_acc:.4f}')\n",
    "    model.train()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second approach for title, overview, poster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "text_train_data = TensorDataset(torch.from_numpy(X_title_overview_train), torch.from_numpy(train_labels))\n",
    "img_train_data = TensorDataset(X_img_train, torch.from_numpy(train_labels))\n",
    "\n",
    "\n",
    "text_val_data = TensorDataset(torch.from_numpy(X_title_overview_val), torch.from_numpy(val_labels))\n",
    "img_val_data = TensorDataset(X_img_val, torch.from_numpy(val_labels))\n",
    "text_test_data = TensorDataset(torch.from_numpy(X_title_overview_test), torch.from_numpy(test_labels))\n",
    "img_test_data = TensorDataset(X_img_test, torch.from_numpy(test_labels))\n",
    "\n",
    "text_train_loader = DataLoader(text_train_data, batch_size=batch_size)\n",
    "img_train_loader = DataLoader(img_train_data, batch_size=batch_size)\n",
    "\n",
    "text_val_loader = DataLoader(text_val_data, batch_size=batch_size)\n",
    "img_val_loader = DataLoader(img_val_data, batch_size=batch_size)\n",
    "\n",
    "text_test_loader = DataLoader(text_test_data, batch_size=batch_size)\n",
    "img_test_loader = DataLoader(img_test_data, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building model for movie genre classification using textual and image features.\n",
    "\n",
    "class CNN_LSTM_title_overview(nn.Module):\n",
    "    def __init__(self, vocab_size, weights_matrix, n_hidden, n_layers, n_out):\n",
    "        super(CNN_LSTM_title_overview, self).__init__()\n",
    "\n",
    "        # LSTM for the text overview\n",
    "        self.vocab_size, self.n_hidden, self.n_out, self.n_layers = vocab_size, n_hidden, n_out, n_layers\n",
    "        num_embeddings, embedding_dim = weights_matrix.shape[0], weights_matrix.shape[1]\n",
    "        self.emb = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.emb.weight.data.copy_(torch.from_numpy(weights_matrix))\n",
    "        self.emb.weight.requires_grad = True\n",
    "        self.lstm = nn.LSTM(embedding_dim, self.n_hidden, self.n_layers, dropout=0.2, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.lstm_fc = nn.Linear(self.n_hidden, 128)\n",
    "        \n",
    "        # CNN for the posters\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3) # Gives me depth of input.\n",
    "        # pooling layers to reduce the image size\n",
    "        self.max_pool1 = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3)\n",
    "        # pooling layers to reduce the image size\n",
    "        self.max_pool2 = nn.MaxPool2d(2)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3)\n",
    "        # pooling layers to reduce the image size\n",
    "        self.max_pool3 = nn.MaxPool2d(2)\n",
    "        self.conv4 = nn.Conv2d(128, 128, 3)\n",
    "        # pooling layers to reduce the image size\n",
    "        self.max_pool4 = nn.MaxPool2d(2)\n",
    "        # to drop the nodes that are below the wanted weights\n",
    "        self.cnn_dropout = nn.Dropout(0.1)\n",
    "        self.cnn_fc = nn.Linear(4*4*128, 512) # Gives me number of features of input.\n",
    "        \n",
    "        # Concat layer for the combined feature space\n",
    "        self.combined_fc1 = nn.Linear(640, 256) # Give me features of input.\n",
    "        self.combined_fc2 = nn.Linear(256, 128)\n",
    "        self.output_fc = nn.Linear(128, n_out)\n",
    "\n",
    "\n",
    "    def forward(self, lstm_inp, cnn_inp):\n",
    "        batch_size = lstm_inp.size(0)\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        lstm_inp = lstm_inp.long()\n",
    "        embeds = self.emb(lstm_inp)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        lstm_out = self.dropout(lstm_out[:, -1])\n",
    "        lstm_out = F.relu(self.lstm_fc(lstm_out))\n",
    "\n",
    "        x = F.relu(self.conv1(cnn_inp))\n",
    "        x = self.max_pool1(x)\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.max_pool2(x)\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.max_pool3(x)\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.max_pool4(x)\n",
    "        #print(x.shape) # that is how I get the correct dimensions for the CNN output that is then passed as input to the linear layer\n",
    "        # need the input to be two dimensional again\n",
    "        x = x.view(-1, 4*4*128)\n",
    "        \n",
    "        x = self.cnn_dropout(x)\n",
    "        cnn_out = F.relu(self.cnn_fc(x))\n",
    "        \n",
    "        #concatenating the cnn output and the lstm output\n",
    "        combined_inp = torch.cat((cnn_out, lstm_out), 1)\n",
    "        # activation function and 2 more linear layers\n",
    "        x_comb = F.relu(self.combined_fc1(combined_inp))\n",
    "        x_comb = F.relu(self.combined_fc2(x_comb))\n",
    "        # sigmoid to get 1 and 0 on the one-hot-encoding formatted label\n",
    "        out = torch.sigmoid(self.output_fc(x_comb))\n",
    "\n",
    "        return out\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device),\n",
    "                          weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device))\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN_LSTM_title_overview(\n",
      "  (emb): Embedding(72673, 300)\n",
      "  (lstm): LSTM(300, 64, num_layers=2, batch_first=True, dropout=0.2)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (lstm_fc): Linear(in_features=64, out_features=128, bias=True)\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (max_pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (max_pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (max_pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (max_pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (cnn_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (cnn_fc): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (combined_fc1): Linear(in_features=640, out_features=256, bias=True)\n",
      "  (combined_fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (output_fc): Linear(in_features=128, out_features=19, bias=True)\n",
      ")\n",
      "cuda:2\n"
     ]
    }
   ],
   "source": [
    "#device = torch.device('cuda:2')\n",
    "vocab_size = len(word_index)+1\n",
    "output_size = train_labels.shape[1]\n",
    "embedding_dim = 300\n",
    "hidden_dim = 64\n",
    "n_layers = 2\n",
    "\n",
    "model = CNN_LSTM_title_overview(vocab_size, word2vec_matrix, hidden_dim, n_layers, output_size)\n",
    "model.to(device)\n",
    "print(model)\n",
    "\n",
    "lr=0.001\n",
    "# criterion = nn.MultiLabelSoftMarginLoss()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done training tensor(0.8238, device='cuda:2')\n",
      "Saving model...\n",
      "Epoch 1: train_loss: 0.3026 train_acc: 0.8238 | val_loss: 0.2866 val_acc: 0.8227\n",
      "done training tensor(0.8335, device='cuda:2')\n",
      "Saving model...\n",
      "Epoch 2: train_loss: 0.2829 train_acc: 0.8335 | val_loss: 0.2821 val_acc: 0.8282\n",
      "done training tensor(0.8378, device='cuda:2')\n",
      "Saving model...\n",
      "Epoch 3: train_loss: 0.2739 train_acc: 0.8378 | val_loss: 0.2740 val_acc: 0.8294\n",
      "done training tensor(0.8464, device='cuda:2')\n",
      "Saving model...\n",
      "Epoch 4: train_loss: 0.2585 train_acc: 0.8464 | val_loss: 0.2661 val_acc: 0.8394\n",
      "done training tensor(0.8554, device='cuda:2')\n",
      "Saving model...\n",
      "Epoch 5: train_loss: 0.2427 train_acc: 0.8554 | val_loss: 0.2534 val_acc: 0.8506\n",
      "done training tensor(0.8658, device='cuda:2')\n",
      "Saving model...\n",
      "Epoch 6: train_loss: 0.2254 train_acc: 0.8658 | val_loss: 0.2462 val_acc: 0.8574\n",
      "done training tensor(0.8737, device='cuda:2')\n",
      "Saving model...\n",
      "Epoch 7: train_loss: 0.2129 train_acc: 0.8737 | val_loss: 0.2541 val_acc: 0.8661\n",
      "done training tensor(0.8804, device='cuda:2')\n",
      "Saving model...\n",
      "Epoch 8: train_loss: 0.2022 train_acc: 0.8804 | val_loss: 0.2477 val_acc: 0.8646\n",
      "done training tensor(0.8857, device='cuda:2')\n",
      "Saving model...\n",
      "Epoch 9: train_loss: 0.1939 train_acc: 0.8857 | val_loss: 0.2562 val_acc: 0.8722\n",
      "done training tensor(0.8909, device='cuda:2')\n",
      "Saving model...\n",
      "Epoch 10: train_loss: 0.1854 train_acc: 0.8909 | val_loss: 0.2607 val_acc: 0.8726\n",
      "done training tensor(0.8952, device='cuda:2')\n",
      "Saving model...\n",
      "Epoch 11: train_loss: 0.1787 train_acc: 0.8952 | val_loss: 0.2634 val_acc: 0.8770\n",
      "done training tensor(0.8991, device='cuda:2')\n",
      "Saving model...\n",
      "Epoch 12: train_loss: 0.1725 train_acc: 0.8991 | val_loss: 0.2775 val_acc: 0.8739\n",
      "done training tensor(0.9024, device='cuda:2')\n",
      "Saving model...\n",
      "Epoch 13: train_loss: 0.1670 train_acc: 0.9024 | val_loss: 0.2638 val_acc: 0.8756\n",
      "done training tensor(0.9055, device='cuda:2')\n",
      "Saving model...\n",
      "Epoch 14: train_loss: 0.1621 train_acc: 0.9055 | val_loss: 0.2748 val_acc: 0.8753\n",
      "done training tensor(0.9086, device='cuda:2')\n",
      "Saving model...\n",
      "Epoch 15: train_loss: 0.1570 train_acc: 0.9086 | val_loss: 0.2820 val_acc: 0.8778\n",
      "done training tensor(0.9118, device='cuda:2')\n",
      "Saving model...\n",
      "Epoch 16: train_loss: 0.1518 train_acc: 0.9118 | val_loss: 0.2915 val_acc: 0.8774\n",
      "done training tensor(0.9144, device='cuda:2')\n",
      "Saving model...\n",
      "Epoch 17: train_loss: 0.1477 train_acc: 0.9144 | val_loss: 0.2897 val_acc: 0.8759\n",
      "done training tensor(0.9173, device='cuda:2')\n",
      "Saving model...\n",
      "Epoch 18: train_loss: 0.1428 train_acc: 0.9173 | val_loss: 0.2939 val_acc: 0.8785\n",
      "done training tensor(0.9197, device='cuda:2')\n",
      "Saving model...\n",
      "Epoch 19: train_loss: 0.1389 train_acc: 0.9197 | val_loss: 0.2967 val_acc: 0.8768\n",
      "done training tensor(0.9225, device='cuda:2')\n",
      "Saving model...\n",
      "Epoch 20: train_loss: 0.1345 train_acc: 0.9225 | val_loss: 0.3056 val_acc: 0.8794\n"
     ]
    }
   ],
   "source": [
    "epochs = 20 # try larger number of epochs\n",
    "clip = 5\n",
    "\n",
    "model.train()\n",
    "for i in range(epochs):\n",
    "    total_acc_train = 0\n",
    "    total_loss_train = 0\n",
    "    \n",
    "    for lstm, cnn in zip(text_train_loader, img_train_loader):\n",
    "        lstm_inp, lstm_labels = lstm\n",
    "        lstm_inp = lstm_inp.float()\n",
    "        cnn_inp, cnn_labels = cnn\n",
    "        cnn_inp = cnn_inp.float()\n",
    "        lstm_inp, lstm_labels = lstm_inp.to(device), lstm_labels.to(device)\n",
    "        cnn_inp, cnn_labels = cnn_inp.to(device), cnn_labels.to(device)\n",
    "        model.zero_grad()\n",
    "        output = model(lstm_inp, cnn_inp)\n",
    "        loss = criterion(output.squeeze(), lstm_labels.float())\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            acc = torch.abs(output.squeeze() - lstm_labels.float()).view(-1)\n",
    "            acc = (1. - acc.sum() / acc.size()[0])\n",
    "            total_acc_train += acc\n",
    "            total_loss_train += loss.item()\n",
    "  \n",
    "    train_acc = total_acc_train/len(text_train_loader)\n",
    "    train_loss = total_loss_train/len(text_train_loader)\n",
    "    print('done training',train_acc)\n",
    "    model.eval()\n",
    "    total_acc_val = 0\n",
    "    total_loss_val = 0\n",
    "    with torch.no_grad():\n",
    "        for lstm, cnn in zip(text_val_loader, img_val_loader):\n",
    "            lstm_inp, lstm_labels = lstm\n",
    "            lstm_inp = lstm_inp.float()\n",
    "            cnn_inp, cnn_labels = cnn\n",
    "            cnn_inp = cnn_inp.float()\n",
    "            lstm_inp, lstm_labels = lstm_inp.to(device), lstm_labels.to(device)\n",
    "            cnn_inp, cnn_labels = cnn_inp.to(device), cnn_labels.to(device)\n",
    "            model.zero_grad()\n",
    "            output = model(lstm_inp, cnn_inp)\n",
    "            val_loss = criterion(output.squeeze(), lstm_labels.float())\n",
    "            acc = torch.abs(output.squeeze() - lstm_labels.float()).view(-1)\n",
    "            acc = (1. - acc.sum() / acc.size()[0])\n",
    "            total_acc_val += acc\n",
    "            total_loss_val += val_loss.item()\n",
    "    print(\"Saving model...\") \n",
    "    torch.save(model.state_dict(), '/home/gusmavko@GU.GU.SE/aics-project/data/model_title_overview_img.pt')\n",
    "\n",
    "    val_acc = total_acc_val/len(text_val_loader)\n",
    "    val_loss = total_loss_val/len(text_val_loader)\n",
    "    print(f'Epoch {i+1}: train_loss: {train_loss:.4f} train_acc: {train_acc:.4f} | val_loss: {val_loss:.4f} val_acc: {val_acc:.4f}')\n",
    "    model.train()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
